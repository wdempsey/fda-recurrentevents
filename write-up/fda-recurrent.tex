\documentclass[11pt]{amsart}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}
\usepackage{amsfonts,amssymb,amsthm, txfonts, pxfonts,amscd}
\usepackage[stretch=10]{microtype}
%\usepackage{hyperref}
\def\struckint{\mathop{%
\def\mathpalette##1##2{\mathchoice{##1\displaystyle##2}%
 {##1\textstyle##2}{##1\scriptstyle##2}{##1\scriptscriptstyle##2}}%
\mathpalette
{\vbox\bgroup\baselineskip0pt\lineskiplimit-1000pt\lineskip-1000pt
\halign\bgroup\hfill$}
{##$\hfill\cr{\intop}\cr\diagup\cr\egroup\egroup}%
}\limits}
\usepackage{natbib}
\usepackage{color}
\usepackage{booktabs,caption,fixltx2e}
\usepackage[flushleft]{threeparttable}

\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}
\usepackage{amsfonts,amssymb,amsthm, txfonts, pxfonts,amscd}
%\usepackage[stretch=10]{microtype}
%\usepackage{hyperref}
%\def\struckint{\mathop{%
%\def\mathpalette##1##2{\mathchoice{##1\displaystyle##2}%
% {##1\textstyle##2}{##1\scriptstyle##2}{##1\scriptscriptstyle##2}}%
%\mathpalette
%{\vbox\bgroup\baselineskip0pt\lineskiplimit-1000pt\lineskip-1000pt
%\halign\bgroup\hfill$}
%{##$\hfill\cr{\intop}\cr\diagup\cr\egroup\egroup}%
%}\limits}

%\newcommand{\sam}[1]{{\color{blue}{#1}}}
\newcommand{\walt}[1]{\textcolor{blue}{[WD:\ #1]}}

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}
\setlength{\topmargin}{-0.25in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
%\setlength{\evensidemargin}{.25in}
%\setlength{\oddsidemargin}{.25in}


\usepackage{setspace}
\doublespacing

\usepackage{soul,color, mathtools, fixmath}
\usepackage{multirow}
\usepackage{enumitem}
\def\pr{\mathop{\text{pr}}\nolimits}

\def\Bka{{\it Biometrika}}
\def\AIC{\textsc{aic}}
\def\T{{ \mathrm{\scriptscriptstyle T} }}
\def\v{{\varepsilon}}
\def\partitionsn{\mathop{\mathcal{P}_{[n]}}\nolimits}
\def\partitionsN{\mathop{\mathcal{P}_{\infty}}\nolimits}
\def\Dmn{\mathop{{D}_{m,n}}\nolimits}
\def\symmetricn{\mathop{\mathcal{S}_{n}}\nolimits}
\def\PE{\mathop{\rm Pitman\mbox{-}Ewens}\nolimits}
\def\per{\mathop{\rm per}\nolimits}
\def\U{\mathop{\mathcal{U}_{}}\nolimits}
\def\Nb{\mathop{\mathbb{N}_{}}\nolimits}
\def\Nbb{\mathop{\mathbf{N}_{}}\nolimits}
\def\nbb{\mathop{\mathbf{n}_{}}\nolimits}
\def\Xbb{\mathop{\mathbf{X}_{}}\nolimits}
\def\fin{\mathop{\text{fin}}\nolimits}
\def\pr{\mathop{\text{pr}}\nolimits}
\def\E{\mathcal{E}}
\def\G{\mathcal{G}}
\def\deg{\text{deg}_{}}
\def\equalinlaw{=_{\mathcal{D}}}
\def\Fk{\mathop{\mathcal{F}_k^{\downarrow}}\nolimits}
\def\F{\mathop{\mathcal{F}^{\downarrow}}\nolimits}
\def\size{\mathop{\text{size}}\nolimits}
\def\Ycong{\mathop{\EY}\nolimits}
\def\EIsig{\mathop{\mathcal{E}_{\mathcal{I}}^{\sigma}}\nolimits}
\def\EY{\mathop{\mathcal{E}_{Y}}\nolimits}
\def\finp{\mathop{\fin(\mathcal{P})}\nolimits}
\def\En{\mathop{\mathfrak{E}_{[n]}}\nolimits}
\def\EN{\mathop{\mathfrak{E}_{\Nb}}\nolimits}
\def\logit{\mathop{\text{logit}_{}}\nolimits}
\def\EI{\mathop{\mathcal{E}_{\mathcal{I}}}\nolimits}
\def\ES{\mathop{\mathfrak{E}_S}\nolimits}
\def\Ifk{\mathop{{I}_{}}\nolimits}
%\def\mathcal{I}{\mathop{\mathcal{I}_{}}\nolimits}
\def\Ycong{\mathop{\EY}\nolimits}
\def\EIsig{\mathop{\mathcal{E}_{\mathcal{I}}^{\sigma}}\nolimits}
\def\EY{\mathop{\mathcal{E}_{Y}}\nolimits}
\def\finp{\mathop{\fin(\mathcal{P})}\nolimits}
\def\En{\mathop{\mathfrak{E}_{[n]}}\nolimits}
\def\EN{\mathop{\mathfrak{E}_{\Nb}}\nolimits}
\def\logit{\mathop{\text{logit}_{}}\nolimits}
\def\EI{\mathop{\mathcal{E}_{\mathcal{I}}}\nolimits}
\def\ES{\mathop{\mathfrak{E}_S}\nolimits}



\DeclareRobustCommand{\citeext}[1]{\citeauthor{#1} (\citeyear{#1})}
\DeclareRobustCommand{\citeint}[1]{(\citeauthor{#1}, \citeyear{#1})}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{meas}[thm]{Measurement}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{assumption}[thm]{Assumption}
\newtheorem{rmk}[thm]{Remark}%\endlocaldefs


% change default numbering for enumerate environment to be in parentheses

\makeatletter

\def\S{\mathcal{S}}
\def\indep{\mathrel{\rlap{$\perp$}\kern1.6pt\mathord{\perp}}}
\def\R{\mathcal{R}}
\def\H{\mathcal{H}}
\def\E{\mathbb{E}}
\def\Y{{\bf Y}}
\def\Cov{\text{Cov}}
\def\one{{\bf 1}}
\def\diag{\text{diag}}
\def\given{\, | \,}
\def\Given{\, \big | \,}
\def\Nat{\mathbb{N}}
\def\Real{\mathbb{R}}
\def\bft{{\bf t}}
\def\bfx{{\bf x}}
\def\bfp{{\bf p}}
\def\bfT{{\bf T}}
\def\bfD{{\bf D}}
\def\dotminussym#1#2{%
  \setbox0=\hbox{$\m@th#1-$}%
  \kern.5\wd0%
  \hbox to 0pt{\hss\hbox{$\m@th#1-$}\hss}%
  \raise.6\ht0\hbox to 0pt{\hss$\m@th#1.$\hss}%
  \kern.5\wd0}
\newcommand{\dotminus}{\mathbin{\mathpalette\dotminussym{}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\mathchardef\mhyphen="2D

% display breaks


\begin{document}


\title[Recurrent event analysis with functional covariates via random subsampling]{Recurrent event analysis in the presence of real-time high frequency data via random subsampling}
\author{Walter Dempsey}
\address {Department of Biostatistics, University of Michigan, 1415 Washington Heights, Ann Arbor, MI 48109, USA}
 \email{wdem@umich.edu}

\date{\today}

\begin{abstract}
Digital monitoring studies collect real-time high frequency data via mobile sensors in the subjects' natural environment. This data can be used to model the impact of changes in physiology on recurrent event outcomes such as smoking, drug use, alcohol use, or self-identified moments of suicide ideation. Likelihood calculations for the recurrent event analysis, however, become computationally prohibitive in this setting. Motivated by this, a random subsampling framework is proposed for computationally efficient, approximate likelihood-based estimation. A subsampling-unbiased estimator for the derivative of the cumulative hazard enters into an approximation of log-likelihood. The estimator has two sources of variation: the first due to the recurrent event model and the second due to subsampling. The latter can be reduced by increasing the sampling rate; however, this leads to increased computational costs.  The approximate score equations are equivalent to logistic regression score equations, allowing for standard, ``off-the-shelf'' software to be used in fitting these models. Simulations demonstrate the method and efficiency-computation trade-off. We then illustrate the approach using data from a digital monitoring study of suicidal ideation.
\end{abstract}

\keywords{recurrent events; probabilistic subsampling; estimating equations; high frequency time series; logistic regression}


\maketitle

\section{Introduction}

Advancement in mobile technology has led to the rapid integration of mobile and wearable sensors into behavioral health~\citep{Freeetal2013}. Take HeartSteps, for example, a mobile health (mHealth) study designed to increase physical activity in sedentary adults \citep{KlasnjaHS2019}. Here, a Jawbone sensor is used to monitor step count every minute of the participant's study day. Of interest in many mHealth studies is the relation of such real-time high frequency sensor data to an adverse, recurrent event process. In a smoking cessation mHealth study~\citep{Sense2Stop}, for example, the relation between a time-varying sensor-based measure of physiological stress and smoking lapse is of scientific interest. In a suicidal ideation mHealth study~\citep{Kleiman2018}, the relation of electrodermal activity (EDA) and accelerometer with self-identified moments of suicidal ideation is of scientific interest.

The goal of this paper is two-fold: (1) to discuss the appropriate choice of statistical model for joint high-frequency sensor data and the recurrent event data, and (2) to construct a simple, easy-to-implement method for parameter estimation and inference. For (1), we discuss an important issue regarding measurement-error models when paired with recurrent event outcomes. For (2), we introduce a random subsampling procedure that has several benefits.  First, the resulting inference is unbiased; however, there is a computation-efficiency trade-off. In particular, a higher sampling rate can decrease estimator variance at the cost of increased computation.  We show via simulations that the benefits of incredibly high sampling rates is often negligible, as the contribution to the variation is small in relation to to variation in the underlying stochastic processes. Second, derived estimating equations are optimal, implying loss of statistical efficiency is only due to subsampling procedure and not derived methodology.  Finally, implementation can leverage existing, standard software for functional data analysis and logistic regression, leading to fast adoption by domain scientists.

\section{Recurrent event process and associated high frequency data}

Suppose $n$ subjects are independently sampled with observed event times~$\bfT_{i} = \{ T_{i,1}, \ldots, T_{i,k_i}\}$ over some observation window $[0, \tau_i]$ for each subject $i = 1,\ldots, n$.  Assume the event times are ordered, i.e., $T_{i,j} < T_{i,j^\prime}$ for $j < j^\prime$. The observation window length, $\tau_i$, is the censoring time and is assumed independent of the event process. Let~$N_{i} (t)$ denote the associated counting process of $\bfT_{i}$; that is, $N_i (t) = \sum_{j=1}^{k_i} 1 [ T_{i,j} < t ]$. In this section, we assume a single-dimensional health process~$\bfx_i = \{ x_i (s) \}_{0 < s < \tau_i}$ for each participant is measured at a dense grid of time points.  Accelerometer, for example, is measured at a rate of 32Hz (i.e., $32$ times per second). Electrodermal activity (EDA), on the other hand, is measured at a rate of 4Hz (i.e., 4 times per second).  Given the high frequency nature of sensor data, this paper assumes the process is measured continuously.

Let~$H_{i,t}^{NX} = H_{i,t}^{N} \otimes H_{i,t}^{X}$ be the $\sigma$-field generated by all past values~$(N_i (s), x_i (s))_{0 \leq s \leq t}$. In this paper, the instantaneous risk of an event at time~$t$ is assumed to depend on the health process, time-in-study, and the event history through a fully parametric conditional hazard function:
\begin{equation}
\label{eq:hazard}
h_i \left( t \Given H_{i,t}^{NX} ; \theta \right) =
\lim_{\delta \to 0} \delta^{-1} \pr \left( N_i(t+\delta) - N_i(t) > 0
  \given H_{i,t}^{NX} \right),
\end{equation}
where~$\theta$ is the parameter vector. Equation~\eqref{eq:hazard} states the hazard may depend on past values of the event history and covariates.  For high frequency physiological data, we assume that current risk is log-additive and depends on a linear functional of the health process over some recent window of time and some pre-specified features of the counting process; that is,
\begin{equation}
\label{eq:hazardlinear}
h_i \left( t \given  H_{i,t}^{NX} ; \theta \right) =
h_0 (t; \gamma) \exp \left( g_t \left( H_{i,t}^{N} \right)^{\prime} \alpha
  + \int_{t-\Delta}^t x_i (s) \beta(s) ds  \right)
\end{equation}
where~$h_0(t;\gamma)$ is a parametrized baseline hazard function,~$\Delta$ is an unknown window-length, and $g_t( H_{i,t}^N ) \in \mathbb{R}^p$ is a $p$-length feature vector summarizing the event-history and time-in-study information. The final term~$\int_{t-\Delta}^t x_i(s) \beta(s) ds$ reflects the unknown linear functional form of the impact of the time-varying covariate on current risk.

An alternative to~\eqref{eq:hazardlinear} would be to construct features from the sensor data history~$f_t ( H_{i,t}^{X}) \in \mathbb{R}^q$ and incorporated these features in the place of the final term. Our current approach builds linear features of~$H_{i,t}^X$ directly from the integrated history, avoiding the feature construction problem -- a highly nontrivial issue for high frequency time-series data.  The main caveat is the additional parameter~$\Delta$; however, as long as the estimated~$\hat \Delta$ exceeds~$\Delta$, then resulting estimation is unbiased albeit at a loss of efficiency.  Moreover, sensitivity analysis can be performed to determine how choice of $\hat \Delta$ affects inference.  One limitation of the approach presented here is that only fully parametric hazard models may be fit to the data. However, a spline model for the log baseline hazard affords sufficient model flexibility.

\subsection{Measurement-error models with event processes}
\label{section:memproblems}

One potential criticism for~\eqref{eq:hazard} is that the health process may be measured with error. A common mathematical strategy for joint models is to consider an unobservable, latent process~$\eta_i$ such that~$\bfT_i \indep x_i \given \eta_i$, i.e., the two processes are conditionally independent given the latent trajectory. For example, take~$\eta_i$ to be a zero-mean Gaussian process with
\begin{equation}\label{eq:jm}
x_i(t) = \eta_i (t) + \epsilon_i (t),\quad \text{ and } \log h_i (t
\given \eta ) = \log h_0 (t) + g_t \left( H_{i,t}^N \right)^{\prime}
\alpha + \int_{t-\Delta}^t \eta_i (s) \beta (s) ds
\end{equation}
where~$\epsilon_i(t)$ is a white-noise measurement error term. Thus, $\eta_i (t)$ is the ``true and unobserved value of the longitudinal outcome'' \citep[Sec. 2.1, pp.3]{Rizopoulos2010}. The conditional survival function of the $j$th event~$T_j$ given~$\bfx_i$ and all prior events~$\bfT_{-j} := \{ T_1, \ldots, T_{j-1} \}$ is
\[
\pr \left ( T_j > t+s \Given \H_{i,\tau_i}^X, \bfT_{-j} = \bft_{-j}, T_j > t \right) = E \left( - \exp \left( \int_{t}^{t+s} h_i (u \given \eta ) du \right) \Given \bfx_i, \bfT_{-j} = \bft_{-j} \right),
\]
where the expectation is a Gaussian integral, albeit infinite-dimensional.  For most choices of covariance structure, if the white-noise error term is non-zero then the above calculation will show the conditional survival function depends not only on past $x$-values, but also on future $x$-values, i.e.,~$\bfx$ and~$\bfT$ do not satisfy \emph{independent evolution}~\citep{DempseyPMCC2}. This is quite unnatural, as~\eqref{eq:jm} suggests the instantaneous risk of an event at time~$t$ depends on future values of the sensor process. To ensure independent evolution, we employ the strategy of setting~$\epsilon (t) \equiv 0$ and treating~$\bfx_i$ as a mean-zero Gaussian process. The proposed inferential procedure will only rely on the Gaussian assumption when~$\bfx$ is not fully observed (see Section~\ref{section:missingdata}).

\subsection{Likelihood calculation}

For the sake of notational simplicity, we leave the dependency of the conditional hazard function on~$H_{i,t}^{NX}$ implicit, and write~$h_i (t; \theta)$. The component of the log-likelihood related to the event process is then given by
\[
  L_n (\theta) = \sum_{i=1}^{n} \left ( \sum_{j=1}^{k_i}
    \log \left( h_i \left( T_{i,j}; \theta
      \right) \right) - H_{i} \left( \tau_i; \theta \right) \right)
\]
where~$H_{i} (\tau_i ; \theta) = \int_{0}^{\tau_i} h_{i} (t; \theta) dt$ is the cumulative hazard function. Solving the associated score equations~$U_n (\theta) = {\bf 0}$ yields the maximum likelihood estimator~$\hat \theta$, where
\[
U_n (\theta) = \sum_{i=1}^{n} \left ( \sum_{j=1}^{k_i} \frac{h^{(1)}_i
    (T_{i,j}; \theta)}{h_i (T_{i,j}; \theta)} - H^{(1)}_{i} (\tau_i;
  \theta) \right)
\]
where~$h_i^{(1)} (T_{i,j}; \theta)$ and $H_i^{(1)} (\tau_{i}; \theta)$ are derivatives with respect to $\theta$.

In classical joint models~\citep{Henderson2000, Tsiatis2004}, time-varying covariates~$x_i (t)$ are observed only intermittently at appointment times. In our current setting, maximizing the likelihood is computationally prohibitive since for any~$\theta$ we must compute the cumulative hazard functions~$H_{i} (\tau_i; \theta)$ which require integration of~$h_i(t;\theta)$ given by~\eqref{eq:hazardlinear} which itself depends on the integral~$\int_{t-\Delta}^t x_i (s) \beta(s) ds$.  That is, the risk model now depends on an integrated past history of the time-varying covariate which leads to severe increase in computational complexity.

\subsection{Probabilistic subsampling framework}

To solve the computational challenge we employ a point-process subsampling design to obtain unbiased estimates of the derivative of the cumulative hazards for each subject. The subsampling procedure treats the collected sensor data as a set of \emph{potential observations}. Suppose covariate information is sampled at times drawn from an independent inhomogeneous Poisson point process with known intensity~$\pi_i (t)$. At a subsampled time~$t$, the \emph{windowed covariate history} $\{ x_i (t-s)\}_{0 \leq s \leq \Delta}$ and counting process features~$g_t (H_{i,t}^N)$ are observed. Optimal choice of~$\pi_i (t)$ is beyond the scope of this paper; however, simulation studies have suggested setting proportional to the hazard function~$h_i (t; \theta)$.

An estimator is design-unbiased if its expectation is equal to that parameter under the probability distribution induced by the sampling design~\citep{Cassel1977}. Let~$\bfD_i \subset [0,t_i]$ denote the set of subsampled points.  Note, by construction, this set is distinct from the set of event times, i.e.,~$\bfT_i \cap \bfD_i = \emptyset$.  Under subsampling via $\pi_i (t)$, one may compute a Horvitz-Thompson estimator of the derivative of the cumulative hazard:
\[
\hat H_{i}^{(1)} (\tau_i; \theta) = \sum_{u \in \bfD_i} \frac{
  h^{(1)}_i (u; \theta) }{ \pi_i (u) }
\]
An alternative design-unbiased estimator of the derivative of the cumulative hazards is given by
\begin{equation}
\label{eq:WPest}
\hat H_{i}^{(1)} (\tau_i; \theta) = \sum_{u \in (\bfT_i \cup \bfD_i)}
\frac{ h^{(1)}_i ( u; \theta ) }{ \pi_i (u) + h_i ( u ; \theta) }
\end{equation}
Equation~\eqref{eq:WPest} is the estimator suggested by~\cite{Waagepetersen2008}.  This estimator depends on the superposition of the event and subsampling processes. Proposition~\ref{prop:optimal} shows the estimator for~$\theta$ associated with using~\eqref{eq:WPest} is the most efficient within a suitable class of estimators for the derivative of the cumulative hazard function (including the Horvitz-Thompson estimator). Therefore, we restrict our attention to~\eqref{eq:WPest} for the remainder of this paper. Letting
\begin{equation}
\label{eq:waage_weights}
w_i (t; \theta) = \frac{\pi_i (t)}{\pi_i (t) + h_i (t ; \theta)},
\end{equation}
the resulting estimating equations can be written as
\begin{equation}
\label{eq:approxscore}
\hat{U}_n (\theta) = \sum_{i=1}^n \left[ \sum_{u \in \bfT_i} w_i(u; \theta)
  \frac{h^{(1)} (u; \theta)}{ h ( u; \theta)}  - \sum_{u \in D_i} w_i(u;
  \theta) \frac{h_i^{(1)} (u; \theta)}{ \pi_i (u) } \right].
\end{equation}
Equation~\eqref{eq:approxscore} are approximate score functions built via plug-in of the design-unbiased estimator of the derivative of the cumulative hazard given in~\eqref{eq:WPest}.

\begin{rmk}[Connection to design-based estimation]
The approximate score equations given by~\eqref{eq:approxscore} arise in design-based inference of point processes.  Design-based inference is common for spatial point processes~\citep{Waagepetersen2008} where the spatial varying covariate is observed at a random sample of locations. It is common in mobile health where ecological momentary assessments~\citep{Rathbun2012,Rathbun2016} are used to randomly sample individuals at various time-points to assess their emotional state.  In the current setting, we leverage these ideas to form a subsampling protocol that can substantially reduce computationally complexity.  Therefore, the purpose is quite different. Moreover, the dependence of the intensity function on the recent history of sensor values leads to additional complications that must be addressed.
\end{rmk}

\section{Longitudinal functional principal components within
  event-history analysis}

Probabilistic subsampling converts the single sensor stream~$\bfx_i$ into a sequence of functions observed repeatedly at sampled times~$\bfD_i$ and event times~$\bfT_i$. Such a data structure is commonly referred to as \emph{longitudinal functional data}~\citep{Xiao2013, GoldSmith2015}. Given the large increase in longitudinal functional data in recent years, corresponding analysis has received much recent attention~\citep{Morris2003, MorrisCarroll2006, Baladandayuthapani2008, Di2009, Greven2010, Staicu2010, ChenMuller2012, LiGuan2014}. Here, we combine work by~\cite{Park2018} and~\cite{Goldsmith2011} to construct a computationally efficient penalized functional method for solving the estimation equations~$\hat U_n (\theta)$.

\subsection{Estimation of the windowed covariate history}

We start by defining~$X(t,s) = x(t-s)$ to be the sensor measurement~$0 \leq s \leq \Delta$ time units prior to time~$t \in \bfT_i \cup \bfD_i$. We use the sandwich smoother~\citep{Xiao2013} to estimate the mean~$\mu_y(s,t) = \E_y [ X(t,s)]$ where the expectation is indexed by whether $t$ is an event ($y=1$) or subsampled ($y=0$) time respectively. Alternative bivariate smoothers exist, such as the kernel-based local linear smoother~\citep{Hastie2009} bivariate tensor product splines~\citep{Wood2006} and the bivariate penalized spline smoother~\citep{MarxEilers2005}. The sandwich smoother was chosen for its computational efficiency and estimation accuracy. We then define~$\tilde X(s,t) = X(s,t) - \hat \mu_y(s,t)$ to be the mean-zero process at each time~$t \in \bfT_i \cup \bfD_i$.

As in~\cite{Park2018}, define the \emph{marginal covariance} by
\[
\Sigma_y (s, s^\prime) = \int_{0}^\tau c_y( (s,T), (s^\prime, T) ) f_y(T) dt.
\]
for~$0 \leq s,s^\prime \leq \Delta$, where~$c_y((s,t), (s^\prime,t))$ is the covariance function of the windowed covariate history~$X(t,\cdot)$ and $f_y(T)$ is the intensity function for event ($y=1$) and subsampled ($y=0$) times respectively. Estimation of~$\Sigma_y$ occurs in two steps. For simplicity, we present the steps for subsampled times (i.e., $y=0$) but the steps are the same for event times as well. First, the pooled sample covariance is calculated at a set of grid points:
\[
\tilde \Sigma_0 (s_r , s_{r^\prime}) = \left( \sum_{i=1}^n \left | \bfD_i \right |\right)^{-1} \left( \sum_{i=1}^n \sum_{t \in \bfD_i} \tilde X (s_r,t) \tilde X (s_{r^\prime}, t) \right).
\]
Due to our concern over independent evolution as discussed in section~\ref{section:memproblems}, we do not assume that each observation is observed with white noise and therefore the diagonal elements of~$\hat \Sigma_0$ are not inflated. Second, the estimator~$\hat \Sigma$ is further smoothed again using the sandwich smoother~\citep{Xiao2013}. Note~\cite{Park2018} smooth the off-diagonal elements, while here we smooth the entire pooled sample covariance matrix. All negative eigenvalues are set to zero to ensure positive semi-definiteness. The result is used as an estimator~$\hat \Sigma_0$ for the pooled covariance $\Sigma_0$.

Next, we take the spectral decomposition of the estimated covariance function; let $\{ \hat \psi^{(0)}_k (s),\hat \lambda^{(0)}_k \}_{k \geq 1}$ be the resulting sequence of eigenfunctions and eigenvalues. The key benefit of the marginal covariance approach is that it allows us to compute a single, time-invariant basis expansion; this reduces the computational burden by avoiding the three dimensional covariance function (i.e., covariance depends on~$t$) and associated spectral decomposition in methods considered by \cite{ChenMuller2012}. Using the Karhunen-Lo{\`e}ve decomposition, we can represent~$X(t,s)$ for $t \in \bfT_i \cap \bfD_i$ by
\[
X(s,t) = \hat \mu_y (s,t) + \sum_{k=1}^{\infty} \hat c^{(y)}_{i,k} (t) \hat
\psi^{(y)}_k (s) \approx \hat \mu_y (s,t) + {\bf c}^{(y)}_{i} (t)^\top \mathbold{\hat
  \psi}^{(y)} (s)
\]
where $\hat c^{(y)}_{i,k} (t) = \int_{t-\Delta}^t \tilde X_i (s,t) \hat \psi^{(y)}_k (s) ds$, ${\bf c}^{(y)}_i (t) = (c^{(y)}_{i,1} (t), \ldots, c^{(y)}_{i,K_x} (t))^\top$, $\mathbold{\hat \psi}^{(y)} (s) = (\hat \psi^{(y)}_1 (s), \ldots, \hat \psi^{(y)}_{K_x} (s))^\top$, and~$K_x < \infty$ is the truncation level of the infinite expansion. Following~\cite{Goldsmith2011}, we set~$K_x$ to satisfy identifiability constraints (see Section~\ref{section:beta} for details). In subsequent sections, we leave the dependence on $y$ (i.e., whether $t \in \bfT_i$ or $\in \bfD_i$) implicit unless required for notational simplicity.


\subsection{Estimation of~$\beta(s)$}
\label{section:beta}

The next step of our method is modeling~$\beta(s)$. Here, we leverage ideas from the penalized spline literature~\citep{Ruppert2003, Wood2006book}. Let~$\mathbold{\phi} (s) = \{ \phi_1 (s), \ldots, \phi_{K_b} (x) \}$ be a spline basis and assume that~$\beta(s) = \sum_{j=1}^{K_b} b_j \phi_{j} (s) = \mathbold{\phi} (t) {\bf b}$ where~${\bf b} = [b_1, \ldots, b_{K_b}]^{\top}$. Thus, the integral in~\eqref{eq:hazardlinear} can be restated as
\begin{align*}
\int_{t-\Delta}^t X(s,t) \beta(s) ds
  &\approx \int_{t-\Delta}^t \left[ \hat \mu(s,t) + {\bf c} (t)^\top
    \mathbold{\hat \psi} (s) \right] \times \left[
    \mathbold{\phi} (s) {\bf b} \right] \\
  &= [ ( M_{i,t} )^\top + (C_{i,t})^\top J_{\hat \psi, \phi} ] {\bf b}
\end{align*}
where~$M_{i,t} = (M_{i,1,t}, \ldots, M_{i,K_b,t})$, $M_{i,j,t} = \int_{t-\Delta}^t \hat \mu (s,t) \phi_j (s)$, and~$J_{\hat \psi, \phi}$ is a $K_x \times K_b$ dimensional matrix with the~$(k,l)$th entry is equal to~$\int_{0}^\Delta \hat \psi_k (s) \phi_l (s) ds$~\citep{RamsaySilverman2005}.

Given the basis for~$\beta(t)$, the model depends on choice of both~$K_b$ and~$K_x$.  We follow~\cite{Ruppert2002} by choosing $K_b$ large enough to prevent under-smoothing and~$K_x \geq K_b$ to satisfy identifiability constraints. While our theoretical analysis considers truncation levels that depend on~$n$, in practice, we follow the simple rule of thumb and set~$K_b = K_x = 35$. As long as the choices of~$K_x$ and $K_b$ are large enough, their impact on estimation is typically negligible. Below, we will exploit a connection between~\eqref{eq:approxscore} and score equations for a logistic regression model.  Before moving on, we introduce some additional notation. Define
\begin{equation}
\label{eq:approx_hazard}
h_i \left( t \given  H_{i,t}^{NX} ; \theta \right) \approx
\exp \left( Z_{t}^\top \gamma + g_t \left( H_{i,t}^{N} \right)^{\prime} \alpha
  + M_{i,t}^\top {\bf b} + C_{i,t}^\top J_{\hat \psi, \phi} {\bf b} \right)
= \exp \left( W_{i,t}^\top \theta \right),
\end{equation}
where~$\theta = (\gamma, \alpha, {\bf b})$ and~$\exp ( Z_{t}^\top \gamma) =: h_0 (t)$ is the parameterized baseline intensity function. We write~$\tilde U_n (\theta)$ to denote the approximate score function when substituting in~\eqref{eq:approx_hazard} for~\eqref{eq:hazardlinear}.

\subsection{Connection to logistic score functions}

We next establish a connection between the above approximate score equations~$\tilde U_n (\theta)$ and the score equations for a logistic regression model. We can then exploit this connection to allow the model to be fit robustly using standard mixed effects software~\citep{Ruppert2002, McCulloch2001}.

\begin{lemma} \normalfont
\label{lemma:logistic}
Under weights~\eqref{eq:waage_weights} and the log-linear intensity function~\eqref{eq:approx_hazard}, the approximate score function~$\tilde U_n (\theta)$ is equivalent to
\[
\sum_{i=1}^n \sum_{t \in \bfT_i \cup D_i } \left[ {\bf 1}[t \in D_i]
  - \frac{1}{1 + \exp \left[- \left( \tilde{W}_{i,t}^\top \theta +
        \log \pi_i (t) \right) \right]} \right] \tilde{W}_{i,t}
\]
where~$\tilde W_{i,t} = -W_{i,t}$. This is the score function for logistic regression with binary response~$Y_i(t)$ for $t \in \bfT_i \cup \bfD_i$ and~$i \in [n]$ where~$Y_i(t) = 1[t \in \bfT_i]$, offset~$\log \pi_i (t)$, and covariates~$\tilde W_{i,t}$.
\end{lemma}

This connection established by Lemma~\ref{lemma:logistic} between our proposed methodology and logistic regression allows us to leverage ``off-the-shelf'' software.  The main complication is pre-processing of the functional data; however, these additional steps can also be taken care of via existing software.  Therefore, the entire data analytic pipeline is easy-to-implement and requires minimal additional effort by the end-user. To see this, we briefly review the proposed inference procedure.

\begin{rmk}[Inference procedure review] \normalfont
Given observed recurrent event and high frequency data~$\{ \bfT_i, \bfx_i \}_{i=1}^n$,
\begin{enumerate}
\item For each $i \in [n]$, sample non-event times as a time-inhomogeneous Poisson point process with intensity according to~$\pi_i (t)$
\item Estimate mean~$\mu_y (t,s)$ for $0 \leq s \leq \Delta$ at all event times~$t \in \cup_{i=1}^n \bfT_i$ and sampled non-event times~$t \in \cup_{i=1}^n \bfD_i$.
\item Compute marginal covariance across event times,~$\Sigma_1$, and non-event times and~$\Sigma_0$.
\item Compute eigendecomposition~$\{ \hat \psi_k^{(y)}, \hat \lambda_k^{()} \}$ of marginal covariance~$\Sigma_y$
\item Use the eigendecomposition to construct~$W_{i,t}$ for all $i \in [n]$ and $t \in \bfD_i \cup \bfT_i$
\item \label{point:log} Perform logistic regression with binary outcome~$\{ \Y_i (t) \}$  and offset of~$\log \pi_i (t)$.
\end{enumerate}
\end{rmk}

\noindent Before demonstrating the methodology via simulation in Section~\ref{section:simstudy} and a worked example in Section~\ref{section:example}, we provide a theoretical analysis of our current proposal.


\subsection{Theoretical analysis}

Our theoretical analysis requires assumptions regarding the subsampling procedure, the event process, and the functional data. We state these assumptions and then our main theorems. We start by assuming there exists a~$\tau < \infty$ such that all individuals are no longer at risk (i.e.,~$\tau_i < \tau$ for all~$i$). Moreover, define $R_i (t)$ to be the at-risk indicator for participant~$i$, i.e., $R_i (t) = {\bf 1} [t \in (0,\tau_i)]$. Asymptotic theory provided in Lemma~\ref{lemma:simpleasym} will be proven under regularity conditions A-E in~\cite[pp. 420--421]{Andersen1993} along with the following additional assumptions:

\begin{assumption}[Event process assumptions]
\label{assumption:events}\normalfont
We assume the following holds:
\begin{enumerate}[label=(E.\arabic*)]
\item\label{E1} The subsampling rate is both lower and upper bounded for all at-risk times; that is,~$0 < L < \pi_i (t) < U < \infty$ for all~$i=1,2,\ldots$ and~$t \in [0,\tau]$ such that $R_i (t) = 1$
\item\label{E2} There exists a nonnegative definite matrix~$\Xi (\theta)$ such that
  \[
    n^{-1} \Xi_n (\theta) = n^{-1} \sum_{i=1}^n \int_0^\tau w_i (t;
    \theta) (1 - w_i (t; \theta)) \left[ \frac{h_i^{(1)}(t;
        \theta)}{h_i (t; \theta)} \right] \times  \left [
      \frac{h_i^{(1)} (t;\theta)}{h_i(t; \theta)} \right]^\top R_i (t)
    dt \overset{P}{\to} \Xi (\theta).
  \]
\item\label{E3} There exists~$M$ such that $|W_{i,j,t}| < M$ for all~$(i,j,t)$.
\item\label{E4} For all~$j,k$
\[
n^{-1} \sum_{i=1}^n \int_0^\tau \left | \frac{d^2}{d\theta_j
    d\theta_k} h_i (t;\theta_0) \right|^2 R_i (t) dt \overset{P}{\to}
C < \infty
\]
as $n \to \infty$.
\end{enumerate}
\end{assumption}

We also require several assumptions due to the truncation of the Karhunen-Lo{\`e}ve decomposition that represents~$X(s,t)$.
\begin{assumption}[Functional assumptions~\citep{Park2018}] \normalfont
\label{assumption:truncation}
The following assumptions are standard in prior work on longitudinal functional data analysis~\citep{Park2018, Yao2005, ChenMuller2012}:
\begin{enumerate}[label=(A.\arabic*)]
\item\label{A1} $X = \{ X(s, t) : (s,t) \in \mathcal{S} \times
  \mathcal{T} \}$ is a square integrable element of the $L^2 (
  \mathcal{S} \times \mathcal{T})$
\item\label{A2} The subsampling and conditional intensity rate functions~$f_y(T)$ are continuous and~$\sup |f_y(T)| < \infty$.
\item\label{A3} $\E[X(s,t) X(s^\prime, t) X(s,t^\prime)
  X(s^\prime, t^\prime) ] < \infty$ for
  each $s,s^\prime \in [0,\Delta]$ and~$0 < t, t^\prime < \tau$.
\item\label{A4} $\E[\|X(\cdot, t)\|^4] < \infty$ for each~$0< t < \tau$.
\end{enumerate}
\end{assumption}
Finally, for simplicity, we assume that there exists~$b^\star$ such that~$\beta(t) = \mathbold{\phi} (t) b^\star$; that is, the true function~$\beta(t)$ sits in the span of the spline basis expansion. With the necessary assumptions stated, we can now state Lemma~\ref{lemma:simpleasym}, which provides asymptotic theory.

Assumption~\ref{A3} is guaranteed by~\ref{E1} and~\ref{E3}, but is stated here to emphasize its importance for the marginal covariance procedure of~\cite{Park2018}. Finally, for simplicity, we assume that there exists~$b^\star$ such that~$\beta(t) = \mathbold{\phi} (t) b^\star$; that is, the true function~$\beta(t)$ sits in the span of the spline basis expansion. With the necessary assumptions stated, we can now state Lemma~\ref{lemma:simpleasym}, which provides asymptotic theory.

\begin{lemma} \normalfont
\label{lemma:simpleasym}
Under Assumption~\ref{assumption:events}, Assumption~\ref{assumption:truncation}, and~$\Delta$ known, for large~$n$ the estimator~$\hat \theta_n$ is consistent; moreover,
\[
\sqrt{n} (\hat \theta - \theta) \to N(0, \Xi (\theta)^{-1})
\]
where
\[
  \Xi (\theta) = \int_{0}^{\tau} w(s; \theta) \left[ \frac{h^{(1)}(s;
      \theta) \times  h^{(1)} (s;\theta)^{\top}}{h(s; \theta)} \right]
  ds.
\]
and~$\tau$ is the random censoring time of the event process.
\end{lemma}
An estimator for~$\Xi(\theta)$ is
\begin{equation}
\label{eq:fisher}
  \hat \Xi (\theta) = n^{-1} \sum_{i=1}^n \sum_{t \in \bfT_i \cup D_i}
  w_i(t;\theta) (1-w_i(t;\theta)) \left[ \frac{h_i^{(1)}(t;
      \theta)}{h_i (t; \theta)} \right] \times  \left [
    \frac{h_i^{(1)} (t;\theta)}{h_i(t; \theta)} \right]^\top
\end{equation}
For the log-linear intensity model, the sampling-unbiased estimator for~$\hat \Xi(\theta)$ is equivalent to the Fisher information for the previously described logistic regression model.
This implies that subsampling from an inhomogeneous Poisson process, standard logistic regression software can be used to fit the recurrent event model by specifying an offset equal to~$\log \pi_i (t)$. Not only this, Lemma~\ref{prop:optimal} shows weights~\eqref{eq:waage_weights} are optimal within a particular class of weighted estimating equations.

\begin{lemma} \normalfont
\label{prop:optimal}
If the event process is an inhomogeneous Poisson point process with intensity~$h(t; \theta)$ and subsampling occurs via an independent, inhomogeneous Poisson point process with intensity~$\pi (t)$, then~$\hat U_n (\theta)$ are optimal estimating functions (i.e., most efficient) in the class of weighted estimating functions given by~\eqref{eq:approxscore} replacing~\eqref{eq:waage_weights} by any weight function~$w_i (t; \theta)$. This class includes the Horvitz-Thompson estimator under~$w(s; \theta) = 1$.
\end{lemma}

\noindent Proposition~\ref{prop:optimal} ensures the only loss of statistical efficiency is due to subsampling and not using a suboptimal estimation procedure given subsampling.
% This result follows from the probability-based sampling design literature~\cite{Rathbun2012}.

\subsection{Computation versus statistical efficiency tradeoff}

Under assumption~\ref{assumption:truncation}, we next consider the statistical efficiency of our proposed estimator when compared to complete-data maximum likelihood estimation. While subsampling introduces additional variation, it may significantly reduce the overall computational burden. It is this trade-off that we next make precise. To start, we consider the subsampling rate~$\pi(t) = c \times h(t; \theta)$ for $c>0$, i.e., the subsampling rate proportional to the intensity function with time-independent constant~$c > 0$. Under this subsampling rate, the weight function~\eqref{eq:waage_weights} is equal to $c / (c+1)$. Under Lemma~\ref{lemma:simpleasym},

For the log-linear intensity model, the sampling-unbiased estimator for~$\hat \Xi(\theta)$ is equivalent to the Fisher information for the previously described logistic regression model. This implies that subsampling from an inhomogeneous Poisson process, standard logistic regression software can be used to fit the recurrent event model by specifying an offset equal to~$\log \pi_i (t)$. Not only this, Lemma~\ref{prop:optimal} shows weights~\eqref{eq:waage_weights} are optimal within a particular class of weighted estimating equations.

\begin{prop} \normalfont
\label{prop:optimal}
If the event process is an inhomogeneous Poisson point process with intensity~$h(t; \theta)$ and subsampling occurs via an independent, inhomogeneous Poisson point process with intensity~$\pi (t)$, then~$\hat U_n (\theta)$ are optimal estimating functions (i.e., most efficient) in the class of weighted estimating functions given by~\eqref{eq:approxscore} replacing~\eqref{eq:waage_weights} by any weight function~$w_i (t; \theta)$. This class includes the Horvitz-Thompson estimator under~$w(s; \theta) = 1$.
\end{prop}

\noindent Proposition~\ref{prop:optimal} ensures the only loss of statistical efficiency is due to subsampling and not using a suboptimal estimation procedure given subsampling.

\subsection{Computation versus statistical efficiency tradeoff}

Under assumption~\ref{assumption:truncation}, we next consider the statistical efficiency of our proposed estimator when compared to complete-data maximum likelihood estimation. While subsampling introduces additional variation, it may significantly reduce the overall computational burden. It is this trade-off that we next make precise. In particular, we consider the following choice of subsampling rate,~$\pi(t) = c \times h(t; \theta)$ for $c>0$. That is, the subsampling rate is proportional to the intensity function with time-independent constant~$c > 0$. Under this subsampling rate, the weight function~\eqref{eq:waage_weights} is equal to $c / (c+1)$. Under Lemma~\ref{lemma:simpleasym},
\[
\Xi (\theta) = \frac{c}{c+1} \int_0^\tau \frac{ h^{(1)} (t; \theta)
  h^{(1)} (t; \theta)^\top}{h (t; \theta)} dt = \frac{c}{c+1} \Sigma (\theta)
\]
where~$\Sigma(\theta)$ is the Fisher information of the complete-data maximum likelihood estimator.
Therefore the relative efficiency is~$c/(1+c)$. For an upper bound~$H = \max_{t \in (0,\tau)} h(t;\theta)$, if we set~$\pi (t) = c \times H$, then the relative efficiency can be lower bounded by~$c / (c+1)$.

Recall sensor measurements occur on the order of multiple times per second.  Suppose the intensity rate is bounded above by $3$ and the unit time scale was hours. If we can subsample the data at~$30$ times per hour, then we have a lower bound on the efficiency of $0.909$. For a 4Hz sensor, this reduces the number of samples per hour from~$4 \times 60 \times 60 = 14,400$ per hour to on average $30$. While the computational complexity of logistic regression is linear in the number of samples, we get $480$ times reduction in the data size at the cost of a $0.909$ statistical efficiency. If we sample~$300$ times per hour, then the efficiency loss is only $0.999$, with a $48$ times reduction in data size. Table~\ref{tab:compvseff} provides additional examples for a 4Hz sensor.  The data reduction depends on the sensor measurement rate; however, the lower bound on statistical efficiency does not.  This is because the subsampling rate only depends on the upper bound of the intensity function. That is, if the events are rare then the amount of data we need to subsample is greatly reduced with no impact to statistical efficiency.

Recall sensor measurements occur on the order of multiple times per second.  Suppose the intensity rate is bounded above by $3$ and the unit time scale was hours. If we can subsample the data at~$30$ times per hour, then we have a lower bound on the efficiency of $0.909$. For a 4Hz sensor, this reduces the number of samples per hour from~$4 \times 60 \times 60 = 14,400$ per hour to on average $30$. While the computational complexity of logistic regression is linear in the number of samples, we get $480$ times reduction in the data size at the cost of a $0.909$ statistical efficiency. If we sample~$300$ times per hour, then the efficiency loss is only $0.999$, with a $48$ times reduction in data size. Table~\ref{tab:compvseff} provides additional examples for a 4Hz sensor.  The data reduction depends on the sensing rate; however, the lower bound on statistical efficiency does not.  This is because the subsampling rate only depends on the upper bound of the intensity function. That is, if the events are rare then the amount of data we need to subsample is greatly reduced with no impact to statistical efficiency.

\begin{table}[!th]
\centering
\begin{tabular}{l r r r r r | c}
\multirow{2}{2.5cm}{Subsampling constant ($c$)}
  & \multicolumn{5}{c}{Upper bound on intensity rate per
    hour}
  & \multirow{2}{2cm}{Statistical efficiency}\\ \cline{2-6}
& 0.5 & 1 & 3 & 5 & 10 \\ \hline
1 & 28800.0 & 14400.0 & 4800.0 & 2880.0 & 1440.0 & 0.500 \\
5 & 5760.0 & 2880.0 & 960.0 & 576.0 & 288.0 & 0.833 \\
10 & 2880.0 & 1440.0 & 480.0 & 288.0 & 144.0 & 0.909 \\
100 & 288.0 & 144.0 & 48.0 & 28.8 & 14.4 & 0.990 \\
500 & 57.6 & 28.8 & 9.6 & 5.8 & 2.9 & 0.999 \\ \hline
\end{tabular}
\caption{Data reduction (total number of measurements divided by expected number of subsampled measurements) given 4Hz sensor, subsampling constant~$c$ and an upper bound on the intensity rate per hour~$H$. The subsampling rate is set at~$c \times H$; note, lower bound on statistical efficiency only depends on the subsampling rate.}
\label{tab:compvseff}
\end{table}

\subsection{Penalized functional regression models}

Recall theoretical results were proven under the assumption that there exists~$b^\star$ such that~$\beta(t) = \phi(t) {\bf b}^\star$. To make this assumption plausible, we set~$K_b$ large enough (but less than~$K_x$ to ensure identifiability) to ensure the spline basis expansion is sufficiently expressive. However, in practice, such a choice of~$K_b$ may lead to overfitting the data. Following~\cite{Goldsmith2011}, we choose the polynomial spline model and set~$\phi(t) = b_0 + b_1 t + b_2 t^2 + \sum_{j=3}^{K_b} (t-\kappa_j)^3$ where~$\{ \kappa_j \}_{j=3}^{K_b}$ are the chosen knots and assume~$\{ b_j \}_{j=3}^{K_b} \sim N(0, \sigma^2 I)$ to induce smoothness on the spline model.  Combining the penalized spline formulation with Lemma~\ref{lemma:logistic} establishes a connection between our approximate score equations and solving a generalized mixed effects logistic regression with offset.

\subsection{Confidence intervals for~$\beta(t)$}

Due to the connection with generalized mixed effects models, we can leverage existing inferential machinery to obtain variance-covariance estimates of model parameters. That is, if~$\hat \Sigma_{bb}$ is the $K_b \times K_b$ dimensional matrix obtained by plugging in the estimates of variance components into the formula for the variance of~$\hat b$, then the standard error for estimate at time~$t_0$ -- i.e., ~$\hat \beta (t_0) = \phi(t_0) \hat {\bf b}$ -- is given by~$\sqrt{ \phi (t_0 ) \hat \Sigma_{bb} \phi(t_0)^\top}$.  Then the approximate 95\% confidence interval can be constructed as~$\hat \beta (t_0) \pm 1.96 \sqrt{\phi (t_0) \hat \Sigma_{bb} \phi(t_0)^\top}$.

Recall theoretical results were proven under the assumption that there exists~$b^\star$ such that~$\beta(t) = \phi(t) {\bf b}^\star$. To make this assumption plausible, we set~$K_b$ large enough (but less than~$K_x$ to ensure identifiability) such that the spline basis expansion is sufficiently expressive. However, in practice, such a choice of~$K_b$ may lead to overfitting the data. We therefore, in the remainder of this paper, follow~\cite{GoldSmith2015} and induce smoothing by assuming that~$\{ b_k \}_{k=3}^{K_b} \sim N(0, \sigma_b^2 I)$. Incorporating penalization is simple due to the connection between with the score functions for logistic regression.

\subsection{Confidence intervals for~$\beta(t)$}

Due to the connection with generalized mixed effects models, we can leverage existing inferential machinery to obtain variance-covariance estimates of model parameters. That is, if~$\hat \Sigma_{bb}$ is the $K_b \times K_b$ deimnsional matrix obtained by plugging in the REML estimates of variance components into the formula for the variance of~$\hat b$, then the standard error for estimate at time~$t_0$ -- i.e., ~$\hat \beta (t_0) = \phi(t_0) \hat {\bf b}$ -- is given by~$\sqrt{ \phi (t_0 ) \hat \Sigma_{bb} \phi(t_0)^\top}$.  Then the approximate 95\% confidence interval can be constructed as~$\hat \beta (t_0) \pm 1.96 \sqrt{\phi (t_0) \hat \Sigma_{bb} \phi(t_0)^\top}$.

We acknowledge two important limitations of confidence intervals obtained via this approach. First, penalization may lead to confidence intervals that perform poorly in regions where~$\hat \beta(t)$ is oversmoothed. Second, we ignore the variability inherent in the longitudinal functional principal component analysis; that is, our estimates ignore the variability in estimation of eigenfunctions~$\hat \psi$ as well as the coefficients~$\hat c_{i,k}(t)$. Joint modeling could be considered as in~\cite{Crainiceanu2010}, however, this is beyond the scope of this article.

\section{Simulation study} \label{section:simstudy}


We next assess the proposed methodology via a simulation study. Here, we assume each individual is observed over five days where each day is defined on the unit interval~$[0,1]$ with $1000$ equally spaced observation times per day. We define~$X(t)$ at the grid of observations as a mean-zero Gaussian process with covariance
\[
  \Sigma (t, t^\prime) =
  \frac{\sigma^2}{\Gamma (v) 2^{\nu-1}}
  \left( \frac{\sqrt{2 \nu} |t-s|}{\rho_t} \right)^{\nu}
  K_v \left( \frac{\sqrt{2 \nu} |t-s|}{\rho_t} \right)
\]
where~$K_v$ is the modified Bessel function of the second kind.  We set~$\nu = 1/2$, $\sigma^2 = 1$, and~$\rho = 0.3$ as well as set~$K_b = K_x = 35$.  For simplicity, we assume~$\Sigma$ is known in computation of the eigendecompositions. Given~$\{ X(t) \}_{0 \leq t \leq 1}$ for a given user-day, we generate event times according a chosen hazard function~$h (t; \theta)$.  To mimic our real data, we set
\[
h(t; \theta) = \exp \left( \theta_0 + \int_{0}^{\Delta} X(t-s) \beta(s) ds \right).
\]
We set~$\Delta = 1/(12*2)$ to mimic a $30$-minute window for a $12$-hour day.  We set~$\theta_0 = \log(5/1000)$ to set a baseline risk of approximately 5 events per day. We consider two choices of~$\beta(s)$: (1) $\beta_0 + \exp(- \beta_1 s) $, and (2) $\beta_1  * \sin \left( 2 \pi \frac{s}{\Delta} - \pi/2 \right)$

We sample non-events at rates of 1 per 12/6/1/0.5 hours.

Since we are primarily interested in accuracy of the final estimates~$\hat \beta(s)$, we report the mean integrated squared error (MISE) of~$\hat \beta (s)$, defined as~$\frac{1}{100} \sum_{j=1}^{100} \int_{0}^{\Delta^\prime} ( \hat \beta_j (s) - \beta(s))^2 ds$ where~$\beta(s) \equiv 0$ for $s > \Delta$.  We define the MISE in this manner to account for settings where~$\Delta$ is unknown. Table~\ref{tab:mise} show the MISE decomposed into the variance and squared biase

\begin{table}[!th]
\begin{tabular}{c | c c c c c}
Sampling rate & 0.5 & 1 & 5 & 25 & 50 \\ \hline
Variance & (10,12) & - & - & - & - \\
Squared Bias & - & - & - & - & -  \\
MISE & - & - & - & - & -  \\ \hline
Type I error & - & - & - & - & -  \\ \hline
\end{tabular}
\caption{Mean-integrated squared error and type I error as a function of sampling rate for $\beta(s)$ given by (1) and (2) respectively.}
\label{tab:mise}
\end{table}

\subsection{Coverage properties}

We simulate the system $1000$ times under $\beta \equiv 0$ and check the type I error.

\subsection{Impact of $\Delta$}

Theorem~XX shows that if we keep~$K_x = K_b = 35$ and therefore have reconstruction error, we can expect a downward bias in parameter estimates. Moreover, if~$\Delta < \Delta^\star$ then we can expect a similar bias in the parameter estimates.  For the former, however, we can simply let~$K_x = K_b$ grow as a function of data size; for the latter, we can see if~$\hat \beta(s)$ is statistically significant for~$s$ near~$\Delta$. If so, we can set~$\Delta$ larger to ensure the condition holds.  In each simulation, we use $\Delta \in (18,21,24,27,30)$.

\begin{table}[!th]
\begin{tabular}{c c c c c c}
& \multicolumn{5}{c}{Sampling rate} \\ \cline{2-6}
$\Delta=$ & 0.5 & 1 & 5 & 25 & 50 \\ \hline
18  & - & - & - & - & -  \\
21 & - & - & - & - & -  \\
24 & - & - & - & - & -  \\
27 & - & - & - & - & -  \\
30 & - & - & - & - & -  \\ \hline
\end{tabular}
\caption{XXX}
\label{tab:mise}
\end{table}


\section{Extensions}

In this section, we demonstrate the flexibility of our approach by exploring extensions in several important directions to ensure these methods are robust for practical use. This section will continue to leverage the connection to generalized functional linear models provided by Lemma~\ref{lemma:logistic}.

\subsection{Missing data}
\label{section:missingdata}
\walt{Question: How to perform multiple imputation for~$X(\cdot,t)$
  and $X(\cdot, t^\prime)$ for~$|t - t^\prime| <\Delta$? That is, how
  do you perform multiple imputation with this overlapping structure?}

A core concern in mHealth is that sensor data can often be missing for intervals of time due to sensor wearing issue (e.g., sensor band becomes loose, turns off accidentally, data packets lost, or battery dies). In the suicidal ideation case study, for example, there are 2139 self-identified moments of distress across all 91 participants. Of these, 1289 event times had complete data for the prior thirty minutes, 1984 had fraction of missing data on a fine grid less than 30\%, and 1998 had fraction of missing data on a fine grid less than 10\%.

Missing data is a critical issue because~$c_{i,k} (t)$ cannot be estimated if~$X(s,t)$ is not observed for all~$s \in [0,\Delta]$. Moreover, we want standard errors to reflect the uncertainty in these coefficients when missing data is prevalent. \cite{Goldsmith2011} suggest using best linear unbiased predictors (BLUP) or posterior modes in the mixed effects model to estimate~$c_{i,k} (t)$; however, one criticism would be when there is substantial variability in these estimates.  To deal with this, \cite{Crainiceanu2010} take a full Bayesian analysis. \cite{Yao2005} introduced PACE as an alternative frequentist method. However,~\cite{Petrovich2018} shows that for sparse, irregular longitudinal, the imputation model should not ignore the outcome variable~$Y_i (t)$.

Here we present an extension of~\cite{Petrovich2018} to our setting by leveraging Lemma~\ref{lemma:logistic} and the marginal covariance estimation procedure to construct a multiple imputation procedure. Let~$\bfx_{i} (t)$ denote incomplete sensor data at time~$t$ (i.e., at times~$\{ s_{i,r} \}_{r=1}^{k_{it}} \}$ in~$[0,\Delta]$. Then
\begin{align}
\label{eq:impute}
\E [ X_i (s,t) \given Y_i(t) = y, \bfx_i (t) ]
  &= \mu_y (s,t) + {\bf a}_{i,t}^\top (s) {\bf B}_{i,t} (\bfx_i (t) -
    \mathbold{\mu}_i (t) ) \\
\text{Cov} \left( X_i (s,t), X_i (s^\prime, t) \given Y_i (t) = y,
  \bfx_i (t) \right)
  &= \Sigma_{t} (s, s^\prime) -
    {\bf a}_{i,t} (s)^\top {\bf B}_{i,t} {\bf a}_{i,t} (s^\prime)
\end{align}
where we have
\[
{\bf a}_{i,t} (s)^\top = \left( \begin{array}{c} \Sigma_t (s_{i,1}, s) \\ \vdots \\
                       \Sigma_t (s_{i,k_{it}}, s) \end{array} \right); \quad
{\bf B}_{i,t}^{-1} = \left(
  \begin{array}{c c c}
    \Sigma_t(s_{i,1}, s_{i,1}) & \Sigma_t(s_{i,1}, s_{i,2}) & \cdots \\
    \Sigma_t(s_{i,1}, s_{i,2}) & \ddots & \vdots \\
    \vdots & \cdots & \Sigma_t (s_{i,k_{it}}, s_{i,k_{it}})
  \end{array} \right),
\]
$\mathbold{\mu}_i (t) = \E [ \bfx_i (t) \given Y_i (t) = y] = \{
\mu_y (s_{j}, t) \}_{j=1}^r$ and~$\mu_y (s,t)$ is the mean of~$X(t,s)$
from group~$y$, and~$\Sigma_t ( s^\prime, s)$ is the covariance
between~$X(s^\prime,t)$ and $X(s,t)$ for $s^\prime, s \in [0, \Delta]$
and~$t \in \mathbb{R}_+$.
These covariances can be estimated using the methods considered by
\cite{ChenMuller2012}.
% Our imputation procedure instead uses the
% marginal covariance~$\Xi$ as a proxy for the true covariance
% function~$\Sigma_t(\cdot, \cdot)$.
% We found this to work well in simulations as~$\Sigma_t$ was relatively
% constant across~$t$; however, this imputation strategy may be
% inappropriate when the covariance structure changes significantly with
% time~$t$.

The imputation strategy generalizes the procedure introduced by~\cite{Petrovich2018} to the longitudinal functional data setting.  This is only possible due to the connection established with logistic regression by Lemma~\ref{lemma:logistic}. We could consider a multiple imputation estimate with~$M$ imputations; the likelihood estimate is given by~$\bar \theta = M^{-1} \sum_{m=1}^M \hat \theta^{(m)}$ where~$\theta^{(m)}$ is the estimate from the~$m$th imputated dataset; the covariance matrix can be estimated by
\[
M^{-1} \sum_{m=1}^M \hat \Xi^{(m)} (\theta) + \frac{1}{M-1}
\sum_{m=1}^M (\hat \theta^{(m)} - \bar \theta) (\hat \theta^{(m)} -
\bar \theta)
\]
where~$\hat \Xi^{(m)}$ is the estimator~\eqref{eq:fisher} for
the~$m$th imputated dataset.

\subsection{Bootstrap and multiple imputation under uncongenial }

Positives: (1) our imputation model is ``approximately'' congenial; (2) our imputatoin model is simple and easy to use.
Negatives: (1) clearly not congenial under many events and/or high sampling rates. (2) difficult when we overlap due to over-sampling.

Congeniality matters because we want to ensure good coverage
Q: Can we derive a variance that provides coverage
We know our imputation models are uncongenial albeit simple to implement.  Impact should be small if events are rare as impute only in ranges where 1,0 labels give you everything.  To be careful, we investigate alternatives and assess impact.

Simple version compared to the von Hippel's boot MI proposal.
\begin{itemize}
\item Bootstrap $B$ times
\item For $b = 1, \ldots, B$, impute $M$ times
\item Let~$\hat \theta_b = M^{-1} \sum_m \hat \theta_{b,m}$
\item Compute $\text{Var}_{BootMI} = (B-1)^{-1} \sum_{b=1}^B (\hat \theta_b - \hat \theta_{BM})^2$ where $\hat \theta_{BM} = B^{-1} \sum_{b=1}^B \hat \theta_b$.
\end{itemize}


\newpage

\subsection{Multilevel models}

\subsection{Multiple sensors and availability}

\subsection{Online estimation}

\section{A worked example: Adolescent psychiatric inpatient mHealth study} \label{section:example}

During a XX month period in 2019, 91 psychiatric inpatients admitted for suicidal risk to Franciscan Children's Hospital  were enrolled in
 with histologically verified liver cirrhosis were recu
those presenting to a psychiatric hospital with suicide ideation
and/or a recent suicide attempt) during a high risk time period (i.e.,
post-hospitalization)

Currently, only 51 patients for whom the initial biopsy could be
reevaluated using more restrictive criteria were available for
analysis.
One of the goals of the study was to assess the association between sensor-based covariate information can used to identify distress early prior in the process of escalation by monitoring physiological correlates of distress -- in particular, electrodermal activity (EDA), a measure of skin conductance, and ``gACC'', a coordinate-free feature built from accelerometer data that measures physical movement. The scientific hypothesis to be tested is whether a spike in EDA and/or gACC

The goal of the sutyd



EDA and ACC sensor data along with buttonpress timestamps were
graciously provided by Evan Kleiman and his study team {\tt
https://kleimanlab.org}.
At the end of the study period, the mortality rate was 292/488, or
approximately 60\%.

The focus here is on two EDA and the  prothrombin index, a composite
blood coagulation index
related to liver function, measured initially at three-month intervals
and subsequently at roughly twelve-month intervals.
The individual prothrombin trajectories are highly variable,
both in forward and in reverse time,
which tends to obscure patterns and trends.
In Figure~\ref{fig:mean_traj}a the mean trajectory is plotted against time from recruitment
for two patient groups placebo/prednisone and censored/not censored.
Naturally, only those patients who are still alive are included in the average
for that time.
Figure~\ref{fig:mean_traj}b shows the same plots in reverse alignment.
While there are certain similarities in the two plots,
the differences in temporal trends are rather striking.
In particular, prothrombin levels in the six months prior to censoring are fairly stable,
which is in marked contrast with levels in the six months prior to failure,
as seen in the lower pair of curves.

Inspection of the graphs for uncensored patients in the right panel of Figure~\ref{fig:mean_traj} suggests
beginning with the simplest revival model in which the sequences
for distinct patients are independent Gaussian with moments

{\bf comparison over $\Delta$ and using imputation}


\subsection{Effect of electrodermal activity on prognosis}
Over a period of 5~years and one month following recruitment,
patient~$u$ had eight appointments with prothrombin values as follows:

\bibliographystyle{plainnat}
\bibliography{si-fda-refs}

\appendix

\section{Derivation of the design-unbiased score equations}

\begin{proof}[Proof of Lemma~\ref{lemma:logistic}]
Recall~$\frac{d}{d\theta} \log h_i (t; \theta) = \frac{h^{(1)}_i (t; \theta)}{h_i (t; \theta)}$. Under the log-linear intensity function given by~\eqref{eq:approx_hazard}, $\frac{d}{d\theta} \log h_i (t; \theta) = W_{i,t}$ and
\[
\frac{d}{d \theta} \log \left( \pi_i (t) + h_i (t;\theta) \right) =
\frac{\exp \left( W_{i,t}^\top \theta \right)}{\pi_i (t) + \exp
  \left( W_{i,t}^\top \theta \right)} W_{i,t}
\]
Therefore,
\begin{align*}
\tilde U_n (\theta)
  &= \sum_{i=1}^n \left \{ \sum_{t \in \bfT_i} W_{i,t} - \sum_{t \in \bfT_i \cup \bfD_i} \frac{\exp \left( W_{i,t}^\top \theta \right)}{\pi_i (t) + \exp \left( W_{i,t}^\top \theta \right)}  W_{i,t} \right \} \\
  &= \sum_{i=1}^n \left \{ \sum_{t \in \bfT_i} \frac{\pi_i (t)}{\pi_i (t) + \exp \left( W_{i,t}^\top \theta \right)} W_{i,t} - \sum_{t \in \bfD_i} \frac{\exp \left( W_{i,t}^\top \theta \right)}{\pi_i (t) + \exp \left( W_{i,t}^\top \theta \right)} W_{i,t} \right \} \\
  &= \sum_{i=1}^n \left \{ \sum_{t \in \bfT_i} w_{i} (t; \theta) W_{i,t} -
    \sum_{t \in D_i} (1 - w_i (t; \theta)) W_{i,t} \right \} \\
  &= - \sum_{i=1}^n \sum_{t \in \bfT_i \cup \bfD_i} \left[ {\bf 1} [t \in \bfD_i]  - w_{i} (t; \theta) \right] W_{i,t} \\
  &= \sum_{i=1}^n \sum_{t \in \bfT_i \cup \bfD_i}
    \left[ {\bf 1} [t \in \bfD_i]  - \frac{1}{1 + \exp\left( - (
          \tilde W_{i,t}^\top \theta + \log (\pi_i (t) ) ) \right)}
    \right] \tilde W_{i,t}.
\end{align*}
where~$\tilde W_{i,t} = - W_{i,t}$. This is exactly the score equation
for logistic regression with offset~$\log \pi_i (t)$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:simpleasym}]
% Lemma~\ref{lemma:simpleasym} will be proven under regularity conditions A-E in~\cite[pp. 420--421]{Andersen1993} along with the following assumptions related to the subsampling rate~$\pi_i (t)$.
We define the joint counting process
\[
M_i (t) = N_i (t) - \int_0^t (h_i (s; \theta) + \pi_i (s)) R_i (s) ds.
\]
where~$N_i (t)$ is the counting process with jumps at all~$t \in \bfT_i \cup \bfD_i$. Asymptotic consistency is guaranteed~\cite[Theorem VI.1.1]{Andersen1993} if
\begin{equation}
\label{eq:andersen1}
n^{-1} \tilde U_n (\theta_0 ) \overset{P}{\to} 0,
\end{equation}
and
\begin{equation}
\label{eq:andersen2}
n^{-1} \frac{d}{d \theta^{\top}} \tilde U_n (\theta) \big |_{\theta =
  \theta_0} = \Xi (\theta),
\end{equation}
and
\begin{equation}
\label{eq:andersen3}
\lim_{n \to \infty} P \left( \left | n^{-1} \frac{d^2}{d \theta_j d
      \theta_k} \tilde U_n (\theta) \right | < M \text{ for all }
  j,k \text{ and all } \theta \in \Theta_0 \right) = 1
\end{equation}
To prove~\eqref{eq:andersen1}, we decompose~$\tilde U_n (\theta_0)$ into
three terms
\[
n^{-1} \tilde U_n (\theta_0) = n^{-1} U_n (\theta_0 )  + n^{-1} \left(
  \hat U_n (\theta_0) - U_n (\theta) \right) + n^{-1} \left(\tilde U_n
  (\theta_0) - \hat U_n (\theta_0) \right)
\]
where, recall,~$\hat U_n (\theta_0)$ are the logistic score equations with $\int_0^\infty X_i(t-s) \beta(s) ds$ and~$\tilde U_n (\theta_0)$ with the approximation~$C_{i,t}^\top J_{\phi, \psi} {\bf b}$. \cite{Andersen1993} (Theorem VI.1.1) show that~$n^{-1} U_n (\theta_0)
\to 0$. The second term
\begin{align*}
n^{-1} \left( \hat U_n (\theta_0) - U_n (\theta_0) \right) =
  \frac{1}{n} \sum_{i=1}^n \int_0^{\tau} \frac{h^{(1)} (s;
  \theta_0)}{\pi_i (s) + h (s;\theta_0)} d M_i (s).
\end{align*}
where~$h(\cdot; \theta_0)$ is given by~\eqref{eq:hazardlinear}. Lenglart's inequality implies the second term converges in probability to zero under Assumptions~\ref{E1} and~\ref{E2}. The third term satisfies
\begin{align*}
  &n^{-1} \left \| \hat U_n (\theta_0) - \tilde U_n (\theta_0) \right \| \\
  \leq & M n^{-1} \sum_{i=1}^n \sum_{t \in \bfT_i \cup \bfD_i} \bigg \| g
         \left( \tilde W_{i,t}^\top \theta  + \sum_{k=K_x+1}^\infty \hat
         c_{i,k} (t) \int_{t-\Delta}^t \hat \psi_k (s) \phi (s)^\top b
         + \log (\pi_i (t) ) \right) - g \left( \tilde W_{i,t}^\top
         \theta + \log (\pi_i (t)) \right) \bigg \| \\
&\leq M n^{-1} \sum_{i=1}^n \sum_{t \in \bfT_i \cup \bfD_i}
    \sup_{x \in \mathbb{R}} \left \| g^\prime \left( x \right)
  \right\| \times \left \| \sum_{k=K_x+1}^\infty \hat c_{i,k} (t)
  \sum_{l=1}^{K_b} \int_{t-\Delta}^t \hat \psi_k (s) \phi_l (s) b_l
  \right \| \\
&= \frac{M}{4} n^{-1} \sum_{i=1}^n \sum_{t \in \bfT_i \cup \bfD_i}
  \times \sum_{l=1}^{K_b} \left \| \int_{t-\Delta}^t
  \left( X(s,t) - \sum_{k=1}^{K_x} \hat c_{i,k} (t) \psi_k (s) \right)
  \phi_l (s) b_l\right \| \\
&\to \frac{M}{4} \int_0^\tau \sum_{l=1}^{K_b} \left \| \int_{t-\Delta}^t
  \left( X(s,t) - \sum_{k=1}^{K_x} \hat c_{i,k} (t) \psi_k (s) \right)
  \phi_l (s) b_l\right \| (f_0(t) + f_1(t)) dt
\end{align*}
where~$M = \sup \| \tilde W_{i,t} \|$,~$g(x) = 1/(1+\exp(-x))$ is the
expit function.
The second inequality is due to the Taylor remainder theorem.
The third is due to~$g^\prime (x) = e^x/(1+e^x)^2 \leq e^0/(1+e^0)^2 =
1/4$, reordering the summation, and rewriting the error term in terms
of the difference between~$X(s,t)$ and the approximation with
truncation level~$K_x$. Letting~$n \to \infty$ allows us to re-write
the outside sums in terms of the integrated difference where the
integral is with respect to the joint event and sampling distributions~$(f_0(t) + f_1 (t))$. Finally, under Assumptions~\ref{assumption:truncation}, \cite{Park2018} show
\[
X(s,t) - \sum_{k=1}^{K_x} \hat c_{i,k} (t) \psi_k (s) \overset{p}{\to} 0
\]
as~$K_x \to \infty$, which implies the third term goes to zero as the truncation level goes to infinity. The same argument can be used in conjunction under assumptions~\ref{E1} and~\ref{E4} to prove~\eqref{eq:andersen2}.

To prove~\eqref{eq:andersen3},
\[
\left | n^{-1}  \right | \leq \frac{1}{n} \sum_{i=1}^n \int_0^\tau H_{in} (t) dN_i
\]
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:optimal}]
Consider the weighted estimating equations given by~\eqref{eq:approxscore}.
Then
\end{proof}


\end{document}

\item {\bf Step 1}: Estimate~$\hat \mu(s,t)$ via sandwich smoother for
  all~$s \in [0,\Delta]$ and~$t \in \cup_{i=1}^n \left( D_i \cup N_i
  \right)$. Define~$\tilde X (s,t) = X(s,t) - \hat \mu(s,t)$ to be a
  mean-zero process for each~$t$.
\item {\bf Step 2}: Compute the pooled sample covariance
\[
\hat \Xi (s , s^\prime) =
\left( \sum_{i=1}^n \left | N_i \cup D_i \right |\right)^{-1}
\left( \sum_{i=1}^n \sum_{t \in N_i \cup D_i} \tilde X (s,t) \tilde X
  (s^\prime, t) \right).
\]
Compute at a fine set of grid points. This is an estimate of
\[
\Xi (s, s^\prime) = \int_{\mathcal{T}} c( (s,T), (s^\prime, T) ) f(T) dT
\]
where~$f(T)$ is a superposition of the event process and the
subsampling process.
\item {\bf Step 3}: Take spectral decomposition~$\{ \hat \phi_k (s),
  \hat \lambda_k \}_{k \geq 1}$. Choose~$K_x \geq K_b$ large enough.
\item {\bf Step 4}: Compute
\[
\hat c_{i,k} (t) = \int_{t-\Delta}^t \tilde X_i (s,t) \hat \phi_k (s) ds
\]
Then
\[
X(s,t) \approx \hat \mu(s,t) + \sum_{k=1}^{K_x} \hat c_{i,k} (t)
\phi_k (s).
\]
\item {\bf Step 5}: Plugging both the spline and KL decomposition we
  have
\begin{align*}
\int_{t-\Delta}^t X(s,t) \beta(s) ds
  &= \int_{t-\Delta}^t \left[ \hat \mu(s,t) + \sum_{k=1}^{K_x}
    c_{i,t,k} \phi_k (s) \right] \times \left[ \sum_{k^\prime=1}^{K_b}
    \beta_k \psi_k (s) \right] \\
  &= [ M_{i,t}^\prime + C_{i,t}^\prime J_{\phi, \psi} ] \beta
\end{align*}
where~$M_{i,t}$ is a vector with~$i$th entry equal
to~$\int_{t-\Delta}^t \hat \mu(s,t) \psi_k (s) ds$, $C_{i,t} =
(c_{i,t,1}, \ldots, c_{i,t,K_x}) \in \mathbb{R}^{K_x \times 1}$,
$\beta = (\beta_1, \ldots, \beta_{K_b}) \in \mathbb{R}^{K_b \times 1}$
and~$J_{\phi, \psi} \in \mathbb{R}^{K_x \times K_b}$ where the~$(i,j)$
entry is given by
\[
\int_{t-\Delta}^t \phi_{i} (s) \psi_{j} (s) ds.
\]
\item {\bf Step 6}: The approximate score equations
\begin{equation}
\label{eq:weighted_esteq}
\hat U_n (\theta) = \sum_{i=1}^n \left \{ \sum_{t \in N_i} w_{i} (t; \theta) W_{i,t} -
    \sum_{u \in D_i} (1 - w_i (u; \theta)) W_{i,u} \right \}
\end{equation}
where
\begin{equation}
\label{eq:waage_weight}
w_i (t; \theta) = \frac{h_i(t;\theta)}{\pi_i (t) + h_i (t;\theta)}.
\end{equation}
Under weights~\eqref{eq:waage_weight} and log-linear intensity
function,~\eqref{eq:weighted_esteq} is the
\end{itemize}

