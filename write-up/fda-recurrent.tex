\documentclass[11pt]{amsart}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}
\usepackage{amsfonts,amssymb,amsthm, txfonts, pxfonts,amscd}
\usepackage[stretch=10]{microtype}
%\usepackage{hyperref}
\def\struckint{\mathop{%
\def\mathpalette##1##2{\mathchoice{##1\displaystyle##2}%
 {##1\textstyle##2}{##1\scriptstyle##2}{##1\scriptscriptstyle##2}}%
\mathpalette
{\vbox\bgroup\baselineskip0pt\lineskiplimit-1000pt\lineskip-1000pt
\halign\bgroup\hfill$}
{##$\hfill\cr{\intop}\cr\diagup\cr\egroup\egroup}%
}\limits}
\usepackage{natbib}
\usepackage{color}
\usepackage{booktabs,caption,fixltx2e}
\usepackage[flushleft]{threeparttable}

\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}
\usepackage{amsfonts,amssymb,amsthm, txfonts, pxfonts,amscd}
%\usepackage[stretch=10]{microtype}
%\usepackage{hyperref}
%\def\struckint{\mathop{%
%\def\mathpalette##1##2{\mathchoice{##1\displaystyle##2}%
% {##1\textstyle##2}{##1\scriptstyle##2}{##1\scriptscriptstyle##2}}%
%\mathpalette
%{\vbox\bgroup\baselineskip0pt\lineskiplimit-1000pt\lineskip-1000pt
%\halign\bgroup\hfill$}
%{##$\hfill\cr{\intop}\cr\diagup\cr\egroup\egroup}%
%}\limits}

%\newcommand{\sam}[1]{{\color{blue}{#1}}}
\newcommand{\walt}[1]{\textcolor{blue}{[WD:\ #1]}}

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}
\setlength{\topmargin}{-0.25in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
%\setlength{\evensidemargin}{.25in}
%\setlength{\oddsidemargin}{.25in}


\usepackage{setspace}
\doublespacing

\usepackage{soul,color, mathtools, fixmath}
\usepackage{multirow}
\def\pr{\mathop{\text{pr}}\nolimits}

\def\Bka{{\it Biometrika}}
\def\AIC{\textsc{aic}}
\def\T{{ \mathrm{\scriptscriptstyle T} }}
\def\v{{\varepsilon}}
\def\partitionsn{\mathop{\mathcal{P}_{[n]}}\nolimits}
\def\partitionsN{\mathop{\mathcal{P}_{\infty}}\nolimits}
\def\Dmn{\mathop{{D}_{m,n}}\nolimits}
\def\symmetricn{\mathop{\mathcal{S}_{n}}\nolimits}
\def\PE{\mathop{\rm Pitman\mbox{-}Ewens}\nolimits}
\def\per{\mathop{\rm per}\nolimits}
\def\U{\mathop{\mathcal{U}_{}}\nolimits}
\def\Nb{\mathop{\mathbb{N}_{}}\nolimits}
\def\Nbb{\mathop{\mathbf{N}_{}}\nolimits}
\def\nbb{\mathop{\mathbf{n}_{}}\nolimits}
\def\Xbb{\mathop{\mathbf{X}_{}}\nolimits}
\def\fin{\mathop{\text{fin}}\nolimits}
\def\pr{\mathop{\text{pr}}\nolimits}
\def\E{\mathcal{E}}
\def\G{\mathcal{G}}
\def\deg{\text{deg}_{}}
\def\equalinlaw{=_{\mathcal{D}}}
\def\Fk{\mathop{\mathcal{F}_k^{\downarrow}}\nolimits}
\def\F{\mathop{\mathcal{F}^{\downarrow}}\nolimits}
\def\size{\mathop{\text{size}}\nolimits}
\def\Ycong{\mathop{\EY}\nolimits}
\def\EIsig{\mathop{\mathcal{E}_{\mathcal{I}}^{\sigma}}\nolimits}
\def\EY{\mathop{\mathcal{E}_{Y}}\nolimits}
\def\finp{\mathop{\fin(\mathcal{P})}\nolimits}
\def\En{\mathop{\mathfrak{E}_{[n]}}\nolimits}
\def\EN{\mathop{\mathfrak{E}_{\Nb}}\nolimits}
\def\logit{\mathop{\text{logit}_{}}\nolimits}
\def\EI{\mathop{\mathcal{E}_{\mathcal{I}}}\nolimits}
\def\ES{\mathop{\mathfrak{E}_S}\nolimits}
\def\Ifk{\mathop{{I}_{}}\nolimits}
%\def\mathcal{I}{\mathop{\mathcal{I}_{}}\nolimits}
\def\Ycong{\mathop{\EY}\nolimits}
\def\EIsig{\mathop{\mathcal{E}_{\mathcal{I}}^{\sigma}}\nolimits}
\def\EY{\mathop{\mathcal{E}_{Y}}\nolimits}
\def\finp{\mathop{\fin(\mathcal{P})}\nolimits}
\def\En{\mathop{\mathfrak{E}_{[n]}}\nolimits}
\def\EN{\mathop{\mathfrak{E}_{\Nb}}\nolimits}
\def\logit{\mathop{\text{logit}_{}}\nolimits}
\def\EI{\mathop{\mathcal{E}_{\mathcal{I}}}\nolimits}
\def\ES{\mathop{\mathfrak{E}_S}\nolimits}



\DeclareRobustCommand{\citeext}[1]{\citeauthor{#1} (\citeyear{#1})}
\DeclareRobustCommand{\citeint}[1]{(\citeauthor{#1}, \citeyear{#1})}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{meas}[thm]{Measurement}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{assumption}[thm]{Assumption}
\newtheorem{rmk}[thm]{Remark}%\endlocaldefs


% change default numbering for enumerate environment to be in parentheses

\makeatletter

\def\S{\mathcal{S}}
\def\indep{\mathrel{\rlap{$\perp$}\kern1.6pt\mathord{\perp}}}
\def\R{\mathcal{R}}
\def\H{\mathcal{H}}
\def\E{\mathbb{E}}
\def\Y{{\bf Y}}
\def\Cov{\text{Cov}}
\def\one{{\bf 1}}
\def\diag{\text{diag}}
\def\given{\, | \,}
\def\Given{\, \big | \,}
\def\Nat{\mathbb{N}}
\def\Real{\mathbb{R}}
\def\bft{{\bf t}}
\def\bfx{{\bf x}}
\def\bfp{{\bf p}}
\def\bfT{{\bf T}}
\def\dotminussym#1#2{%
  \setbox0=\hbox{$\m@th#1-$}%
  \kern.5\wd0%
  \hbox to 0pt{\hss\hbox{$\m@th#1-$}\hss}%
  \raise.6\ht0\hbox to 0pt{\hss$\m@th#1.$\hss}%
  \kern.5\wd0}
\newcommand{\dotminus}{\mathbin{\mathpalette\dotminussym{}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\mathchardef\mhyphen="2D

% display breaks


\begin{document}


\title{Recurrent event analysis in the presence of real-time high
  frequency data via random subsampling}
\author{Walter Dempsey}
\address {Department of Statistics, 
  Harvard University, One Oxford Street
   Cambridge, MA  02138, USA}
 \email{wdempsey@fas.harvard.edu}

\date{\today}

\begin{abstract}
Digital monitoring studies collect real-time high frequency data via
mobile sensors in the subjects' natural environment.  
This data can be used to model the impact of changes in physiology on
recurrent event outcomes such as smoking, drug use, alcohol use, or
self-identified moments of suicide ideation. 
Likelihood calculations for the recurrent event analysis, however,
become computationally prohibitive in this setting.  
% Unlike classic joint models, event risk is a function of
% recent sensor data, not simply current sensor values.  
Motivated by this, a random subsampling framework is proposed for
computationally efficient, approximate likelihood-based estimation.
A subsampling-unbiased estimator for the derivative of the cumulative
hazard enters into an approximation of log-likelihood. The estimator
has two sources of variation: the first due to the recurrent event
model and the second due to subsampling. The latter can be reduced by
increasing the sampling rate; however, this leads to increased
computational costs.  The approximate score equations are equivalent
to logistic regression score equations, allowing for standard,
``off-the-shelf'' software to be used in fitting these models.
Simulations demonstrate the method and efficiency-computation
tradeoff. We then illustrate the approach using data from a digital
monitoring study of suicide ideation.
\end{abstract}

\keywords{recurrent events; probabilistic subsampling; estimating equations;
high frequency time series; logistic regression}


\maketitle

\section{Introduction}

Advancement in mobile technology has led to the rapid integration of
mobile and wearable sensors into behavioral health~\citep{Freeetal2013}.
Take HeartSteps, for example, a mobile health (mHealth) study designed
to increase physical activity in sedentary adults
\citep{KlasnjaHS2019}. Here, a Jawbone sensor is used to monitor step
count every minute of the participant's study day.
Of interest in many mHealth studies is the relation of such
real-time high frequency sensor data to an adverse, recurrent event
process. In a smoking cessation mHealth study~\citep{Sense2Stop}, for
example, the relation between a time-varying sensor-based measure of
physiological stress and smoking lapse is of scientific interest.
In a suicidal ideation mHealth study~\citep{Kleiman2018}, the relation of
electrodermal activity (EDA) and accelerometer with self-identified
moments of suicidal ideation is of scientific interest.


The goal of this paper is two-fold: (1) to discuss the appropriate
choice of statistical model for joint high-frequency sensor data and
the recurrent event data, and (2) to construct a robust,
easy-to-implement method for parameter estimation and inference. 
For (1), we discuss an important issue regarding measurement-error
models when paired with recurrent event outcomes.
For (2), we introduce a random subsampling procedure that has
several benefits.  First, the resulting inference is unbiased;
however, there is a computation-efficiency trade-off. In particular, a
higher sampling rate can decrease estimator variance at the cost of
increased computation.  We show via simulations that the benefits of
incredibly high sampling rates is often negligible, as the
contribution to the variation is small in relation to to variation in
the underlying stochastic processes. Second, derived estimating
equations are optimal, implying loss of statistical efficiency is only
due to subsampling procedure and not derived methodology.  Finally,
implementation can leverage existing, standard software for functional
data analysis and logistic regression, leading to fast adoption by
domain scientists.  

\section{Recurrent event process and associated high frequency data}

Suppose $n$ subjects are independently sampled with observed event
times~$\bfT_{i} = \{ T_{i,1}, \ldots, T_{i,k_i}\}$ over some observation
window $[0, \tau_i]$ for each subject $i = 1,\ldots, n$.  Assume the
event times are ordered, i.e., $T_{i,j} < T_{i,j^\prime}$ for $j <
j^\prime$. The observation window length, $\tau_i$, is the censoring
time and is assumed independent of the event process. Let~$N_{i} (t)$
denote the associated counting process of $\bfT_{i}$; that is, $N_i
(t) = \sum_{j=1}^{k_i} 1 [ T_{i,j} < t ]$.  
In this section, we assume a single-dimensional health process~$\bfx_i
= \{ x_i (s) \}_{0 < s < \tau_i}$ for each participant is measured at
a dense grid of time points.  Accelerometer, for example, is measured
at a rate of 32Hz (i.e., $32$ times per second). Electrodermal
activity (EDA), on the other hand, is measured at a rate of 4Hz (i.e.,
4 times per second).  Given the high frequency nature of sensor data,
this paper assumes the process is measured continuously.   

Let~$H_{i,t}^{NX} = H_{i,t}^{N} \otimes H_{i,t}^{X}$ be the $\sigma$-field
generated by all past values~$(N_i (s), x_i (s))_{0 \leq s \leq t}$.
The instantaneous risk of an event at time~$t$ is assumed to depend on
the health process, time-in-study, and the event history through a
fully parametric conditional hazard function:
\begin{equation}
\label{eq:hazard}
h_i \left( t \Given H_{i,t}^{NX} ; \theta \right) =
\lim_{\delta \to 0} \delta^{-1} \pr \left( N_i(t+\delta) - N_i(t) > 0
  \given H_{i,t}^{NX}; \theta \right), 
\end{equation}
where~$\theta$ is the parameter vector. Equation~\eqref{eq:hazard}
states the hazard may depend on past values of the event history and
covariates.  For high frequency physiological data, we assume that 
current risk is log-additive and depends on a linear functional of the
health process over some recent window of time and some pre-specified
features of the counting process; that is,  
\begin{equation}
\label{eq:hazardlinear}
h_i \left( t \given  H_{i,t}^{NX} ; \theta \right) = 
h_0 (t) \exp \left( g_t \left( H_{i,t}^{N} \right)^{\prime} \alpha
  + \int_{t-\Delta}^t x_i (s) \beta(s) ds  \right)
\end{equation}
where~$\Delta$ is an unknown window-length, and $g_t( H_{i,t}^N ) \in
\mathbb{R}^p$ is a $p$-length feature vector summarizing the
event-history and time-in-study information. The final
term~$\int_{t-\Delta}^t x_i(s) \beta(s) ds$ reflects the unknown
linear functional form of the impact of the time-varying covariate on
current risk.   

An alternative to~\eqref{eq:hazardlinear} would be to construct
features from the sensor data history~$f_t ( H_{i,t}^{X}) \in
\mathbb{R}^q$ and incorporated these features in the place of the
final term. Our current approach builds linear features of~$H_{i,t}^X$
directly from the integrated history, avoiding the feature
construction problem -- a highly nontrivial issue for high frequency
time-series data.  The main caveat is the additional
parameter~$\Delta$; however, as long as the estimated~$\hat \Delta$
exceeds~$\Delta$, then resulting estimation is unbiased albeit at a
loss of efficiency.  Moreover, sensitivity analysis can be peformed to
determine how choice of $\hat \Delta$ affects inference.  One
limitation of the approach presented here is that only fully
parametric hazard models may be fit to the data. However, a spline
model for the log baseline hazard affords sufficient model
flexibility. 

\subsection{Measurement-error models with event processes}
\label{section:memproblems}

One potential criticism for~\eqref{eq:hazard} is that the health
process may be measured with error.  
A common mathematical strategy for joint models is to consider an
unobservable, latent process~$\eta_i$ such that~$\bfT_i \indep x_i
\given \eta_i$. Take~$\eta_i$ to be a zero-mean Gaussian process with 
\begin{equation}\label{eq:jm}
x_i(t) = \eta_i (t) + \epsilon_i (t),\quad \text{ and } \log h_i (t
\given \eta ) = \log h_0 (t) + g_t \left( H_{i,t}^N \right)^{\prime}
\alpha + \int_{t-\Delta}^t \eta_i (s) \beta (s) ds
\end{equation}
where~$\epsilon (t)$ is a white-noise measurement error. Thus,
$\eta_i (t)$ is the ``true and unobserved value of the longitudinal
outcome'' \citep[Sec. 2.1, pp.3]{Rizopoulos2010}. The conditional
density function of the event process~$\bfT_i$ given~$\bfx_i$ is 
\[
\pr \left ( \bfT_i = \bft \Given \H_{i,\tau_i}^X \right) = E \left(
  \prod_{j=1}^{k_i} h_i (t_j \given \eta) \exp \left(
    \int_{0}^{\tau_i} h_i (s \given \eta ) ds \right) \given \bfx_i \right).
\]
Therefore, the conditional density function depends not only on past
$x$-values, but also on future $x$-values.
That is, the processes~$\bfx$ and~$\bfT$ do not satisfy
\emph{independent evolution}~\citep{DempseyPMCC2}. 
This is quite unnatural, as~\eqref{eq:jm} suggests the instantaneous
risk of an event at time~$t$ depends on future values of the sensor
process. To alleviate this issue, we employ the strategy of
taking~$\epsilon (t) \equiv 0$ and treating~$\bfx_i$ as a mean-zero
Gaussian process. The proposed inferential procedure only relies on
the Gaussian assumption when~$\bfx$ is observed with missingness (see 
Section~\ref{section:missingdata}).

\subsection{Likelihood calculation}  

For the sake of notational simplicity, we leave the dependency of the
conditional hazard function on~$H_{i,t}^{NX}$ implicit, and write~$h_i
(t; \theta)$. The component of the log-likelihood related to the event
process is then given by
\[
  L_n (\theta) = \sum_{i=1}^{n} \left ( \sum_{j=1}^{k_i} 
    \log \left( h_i \left( T_{i,j}; \theta
      \right) \right) - H_{i} \left( \tau_i; \theta \right) \right) 
\]
where~$H_{i} (\tau_i ; \theta) = \int_{0}^{\tau_i} h_{i} (t; \theta)
dt$. Solving the associated score equations~$U_n (\theta) = {\bf 0}$
yields the maximum likelihood estimator~$\hat \theta$, where
\[
U_n (\theta) = \sum_{i=1}^{n} \left ( \sum_{j=1}^{k_i} \frac{h^{(1)}_i
    (T_{i,j}; \theta)}{h_i (T_{i,j}; \theta)} - H^{(1)}_{i} (\tau_i;
  \theta) \right)  
\]
where~$h_i^{(1)} (T_{i,j}; \theta)$ and $H_i^{(1)} (\tau_{i}; \theta)$
are derivatives with respect to $\theta$.

In classical joint models~\citep{Henderson2000, Tsiatis2004},
time-varying covariates~$x_i (t)$ are observed only intermittently at
appointment times. In our current setting, maximizing the likelihood
is computationally prohibitive since for any~$\theta$ we must compute
the cumulative hazard functions~$H_{i} (\tau_i; \theta)$ which require
integration of~$h_i(t;\theta)$ given by~\eqref{eq:hazardlinear} which
itself depends on the integral~$\int_{t-\Delta}^t x_i (s) \beta(s)
ds$.  That is, the risk model now depends on an integrated past
history of the time-varying covariate which leads to severe increase
in computational complexity. 

\subsection{Probabilistic subsampling framework}

To solve the computational challenge we employ a point-process
subsampling design to obtain unbiased estimates of the derivative of
the cumulative hazards for each subject. The subsampling procedure
treats the collected sensor data as a set of \emph{potential
  observations}. Suppose covariate information is sampled at times
drawn from an independent inhomogeneous Poisson point process with
known intensity~$\pi_i (t)$. At a subsampled time~$t$, the
\emph{windowed covariate history} $\{ x_i (t-s)\}_{0 \leq s \leq
  \Delta}$ and counting process features~$g_t (H_{i,t}^N)$ are
observed. Optimal choice of~$\pi_i (t)$ is beyond the scope of this
paper; however, simulation studies have suggested setting proportional
to the hazard function~$h_i (t; \theta)$. 

An estimator is design-unbiased if its expectation is equal to that
parameter under the probability distribution induced by the sampling
design~\citep{Cassel1977}. Let~$D_i \subset [0,t_i]$ denote the set of
subsampled points.  Note, by design, this set is distinct from the set
of event times, i.e.,~$\bfT_i \cap D_i = \emptyset$.  Under
subsampling via $\pi_i (t)$, one may compute a Horvitz-Thompson
estimator of the derivative of the cumulative hazard:
\[
\hat H_{i}^{(1)} (\tau_i; \theta) = \sum_{u \in D_i} \frac{
  h^{(1)}_i (u; \theta) }{ \pi_i (s) }
\]
An alternative design-unbiased estimator of the derivative of the
cumulative hazards is given by
\begin{equation}
\label{eq:WPest}
\hat H_{i}^{(1)} (\tau_i; \theta) = \sum_{u \in (\bfT_i \cup D_i)}
\frac{ h^{(1)}_i ( t; \theta ) }{ \pi_i (s) + h_i ( t ; \theta) }
\end{equation}
Equation~\eqref{eq:WPest} is the estimator suggested
by~\cite{Waagepetersen2008}.  This estimator depends on the
superposition of the event and subsampling processes.
Proposition~\ref{prop:optimal} shows the estimator for~$\theta$
associated with using~\eqref{eq:WPest} is the most efficient within a
suitable class of estimators for the derivative of the cumulative
hazard function (including the Horvitz-Thompson estimator).
Therefore, we restrict our attention to~\eqref{eq:WPest} for the
remainder of this paper. Letting
\begin{equation}
\label{eq:waage_weights}
w_i (t; \theta) = \frac{\pi_i (t)}{\pi_i (t) + h_i (t ; \theta)},
\end{equation}
the resulting estimating equations can be written as
\begin{equation}
\label{eq:approxscore}
\hat{U}_n (\theta) = \sum_{i=1}^n \left[ \sum_{u \in \bfT_i} w_i(u; \theta)
  \frac{h^{(1)} (u; \theta)}{ h ( t; \theta)}  - \sum_{u \in D_i} w_i(u;
  \theta) \frac{h_i^{(1)} (t; \theta)}{ \pi_i (t) } \right].
\end{equation}
Equation~\eqref{eq:approxscore} are approximate score functions built
via plug-in of the design-unbiased estimator of the derivative of the
cumulative hazard given in~\eqref{eq:WPest}.

\section{Longitudinal functional principal components within
  event-history analysis}

Probabilistic subsampling converts the single sensor stream~$\bfx_i$
into a sequence of functions observed repeatedly at sampled
times~$D_i$ and event times~$\bfT_i$. Such a data structure is
commonly referred to as \emph{longitudinal functional
  data}~\citep{Xiao2013, GoldSmith2015}.
Given the large increase in longitudinal functional data in recent
years, corresponding analysis has received much recent
attention~\citep{Morris2003, MorrisCarroll2006,
  Baladandayuthapani2008, Di2009, Greven2010, Staicu2010,
  ChenMuller2012, LiGuan2014}.
Here, we combine work by~\cite{Park2018} and~\cite{Goldsmith2011} to
construct a computationally efficient penalized functional method for
solving the estimation equations~$\hat U_n (\theta)$.

\subsection{Estimation of the windowed covariate history}

We start by defining~$X(t,s) = x(t-s)$ to be the sensor measurement~$0
\leq s \leq \Delta$ time units prior to time~$t \in \bfT_i \cup D_i$.
We use the sandwich smoother~\citep{Xiao2013} to estimate the
mean~$\mu(s,t) = \E [ X(t,s)]$.
Alternative bivariate smoothers exist, such as the kernel-based local
linear smoother~\citep{Hastie2009} bivariate tensor product
splines~\citep{Wood2006} and the bivariate penalized spline
smoother~\citep{MarxEilers2005}.
The sandwich smoother was chosen for its computational efficiency and
estimation accuracy.
We then define~$\tilde X(s,t) = X(s,t) - \hat \mu(s,t)$ to be the
mean-zero process at each time~$t \in \bfT_i \cup D_i$.

As in~\cite{Park2018}, define the \emph{marginal covariance} by
\[
\Sigma (s, s^\prime) = \int_{0}^\tau c( (s,T), (s^\prime, T) ) g(T) dt.
\]
for~$0 \leq s,s^\prime \leq \Delta$, where~$c((s,t), (s^\prime,t))$ is
the covariance function of the windowed covariate history~$X(t,\cdot)$
and $g(T)$ is the joint sampling density of event and subsampled
timepoints. 
Estimation of~$\Sigma$ occurs in two steps. 
First, the pooled sample covariance is calculated at a set of grid
points:
\[
\tilde \Sigma (s_r , s_{r^\prime}) = \left( \sum_{i=1}^n \left | N_i 
    \cup D_i \right |\right)^{-1} \left( \sum_{i=1}^n \sum_{t \in N_i
    \cup D_i} \tilde X (s_r,t) \tilde X (s_{r^\prime}, t) \right).
\]
Due to our concern over independent evolution as discussed in
section~\ref{section:memproblems}, we do not assume that each
observation is observed with white noise and therefore the diagonal
elements of~$\hat \Sigma$ are not inflated. Second, the
estimator~$\hat \Sigma$ is further smoothed again using the sandwich 
smoother~\citep{Xiao2013}. 
Note~\cite{Park2018} smooth the off-diagonal elements, while here we
smooth the entire pooled sample covariance matrix. All negative
eigenvalues are set to zero to ensure positive semi-definiteness.
The result is used as an estimator~$\hat \Sigma$ for the pooled
covariance $\Sigma$.

Next, we take the spectral decomposition of the estimated covariance
function; let~$\{ \hat \psi_k (s),\hat \lambda_k \}_{k \geq 1}$ be the
resulting sequence of eigenfunctions and eigenvalues. 
The key benefit of the marginal covariance approach is that it allows
us to compute a single, time-invariant basis expansion; this reduces
the computational burden by avoiding the three dimensional covariance
function (i.e., covariance depends on~$t$) and associated spectral
decomposition in methods considered by \cite{ChenMuller2012}.
Using the Karhunen-Lo{\`e}ve decomposition, we can represent~$X(t,s)$
by
\[
X(s,t) = \hat \mu(s,t) + \sum_{k=1}^{\infty} \hat c_{i,k} (t) \hat
\psi_k (s) \approx \hat \mu(s,t) + {\bf c}_{i} (t)^\top \mathbold{\hat
  \psi} (s)
\]
where $\hat c_{i,k} (t) = \int_{t-\Delta}^t \tilde X_i (s,t) \hat
\psi_k (s) ds$, ${\bf c}_i (t) = (c_{i,1} (t), \ldots, c_{i,K_x}
(t))^\top$, $\mathbold{\hat \psi} (s) = (\hat \psi_1 (s), \ldots, \hat
\psi_{K_x} (s))^\top$, and~$K_x < \infty$ is the truncation level of
the infinite expansion. Following~\cite{Goldsmith2011}, we set~$K_x$
to satisfy identifiability constraints (see Section~\ref{section:beta}
for details).  


\subsection{Estimation of~$\beta(s)$}
\label{section:beta}

The next step of our method is modeling~$\beta(s)$. Here, we leverage
ideas from the penalized spline literature~\citep{Ruppert2003,
  Wood2006book}. Let~$\mathbold{\phi} (s) = \{ \phi_1 (s), \ldots,
\phi_{K_b} (x) \}$ be a spline basis such that~$\beta(s) =
\sum_{j=1}^{K_b} b_j \phi_{j} (s) = \mathbold{\phi} (t) {\bf b}$
where~${\bf b} = [b_1, \ldots, b_{K_b}]^{\top}$. Thus, the integral
in~\eqref{eq:hazardlinear} can be restated as 
\begin{align*}
\int_{t-\Delta}^t X(s,t) \beta(s) ds 
  &= \int_{t-\Delta}^t \left[ \hat \mu(s,t) + {\bf c} (t)^\top
    \mathbold{\hat \psi} (s) \right] \times \left[
    \mathbold{\phi} (s) {\bf b} \right] \\
  &= [ M_{i,t}^\prime + C_{i,t}^\prime J_{\psi, \phi} ] {\bf b}
\end{align*}
where~$M_{i} (t) = (M_{i,1} (t), \ldots, M_{i,K_b} (t))$, $M_{i,j} (t)
= \int_{t-\Delta}^t \hat \mu (s,t) \phi_j (s)$, and~$J_{\psi, \phi}$ is a
$K_x \times K_b$ dimensional matrix with the~$(k,l)$th entry is equal
to~$\int_{0}^\Delta \psi_k (s) \phi_l (s)
ds$~\citep{RamsaySilverman2005}.

Given the basis for~$\beta(t)$, the model depends on choice of
both~$K_b$ and~$K_x$.  We follow~\cite{Ruppert2002} by choosing $K_b$
large enough to prevent undersmoothing and~$K_x \geq K_b$ to satisfy
identifability constraints.
In this paper, we follow the simple rule of thumb and set~$K_b = K_x =
35$.
As long as the choices of~$K_x$ and $K_b$ are large enough, their
impact on estimation is typically negligible. 
We formalize this statement in Theorem~\ref{thm:truncation}.
Note the choice of~$K_b$ large may lead to overfitting the data.
We therefore follow~\cite{GoldSmith2015} and induce smoothing by
assuming that~$\{ b_k \}_{k=3}^{K_b} \sim N(0, \sigma_b^2 I)$.
Below, we will exploit a connection between~\eqref{eq:approxscore}
and score equations for a logistic regression model.  Before moving
on, we introduce notation:
\begin{equation*}
h_i \left( t \given  H_{i,t}^{NX} ; \theta \right) \approx
\exp \left( Z_{i,t}^\top \gamma + g_t \left( H_{i,t}^{N} \right)^{\prime} \alpha
  + M_{i,t}^\top {\bf b} + C_{i,t}^\top J_{\phi, \psi} {\bf b} \right)
= \exp \left( W_{i,t}^\top \theta \right),
\end{equation*}
where~$\theta = (\gamma, \alpha, {\bf b})$ and~$\exp ( Z_{i,t}^\top
\gamma) =: h_0 (t)$ is the parametrized baseline intensity function.


\subsection{Connection to logistic score functions}

We next establish a connection between the above approximate score 
equations and the score equations for a logistic regression model.
All proofs are provided in the supplementary materials.
We can then exploit this connection to allow the model to be fit
robustly using standard mixed effects software~\citep{Ruppert2002,
  McCulloch2001}.

\begin{lemma} \normalfont
\label{lemma:logistic}
Under weights~\eqref{eq:waage_weights} and the log-linear intensity
function~\eqref{eq:hazardlinear}, the approximate score
function~\eqref{eq:approxscore} is equivalent to 
\[
\sum_{i=1}^n \sum_{t \in \bfT_i \cup D_i } \left[ {\bf 1}[t \in D_i]
  - \frac{1}{1 + \exp \left[- \left( \tilde{W}_{i,t}^\top \theta +
        \log \pi_i (t) \right) \right]} \right] \tilde{W}_{i,t} 
\]
where~$\tilde W_{i,t} = -W_{i,t}$. This is the score function for
logistic regression with binary response~$Y_i(t)$ for $t \in N_i \cup
D_i$ and~$i \in [n]$ where~$Y_i(t) = 1[t \in N_i]$, offset~$\log \pi_i
(t)$, and covariates~$\tilde W_{i,t}$. 
\end{lemma}

In this section, we make the following strong assumption.

\begin{assumption} \normalfont
\label{assumption:truncation}
Both the functional covariate~$\{ X(t,s) \}_{0\leq s \leq \Delta}$ and
the functional coefficient~$\beta(t)$ sit in the span of their
respective truncated basis expansions. That is, there
exists~$b^\star$ such that~$\beta(t) = \mathbold{\phi} (t) b^\star$
and~$X(s,t) = \hat \mu (s,t) + {\bf c}_i (t)^\top \mathbold{\hat \psi}
(s)$.
\end{assumption}

Assumption~\ref{assumption:truncation} allows us to ignore the impact
of truncation and present Lemma~\ref{lemma:simpleasym} which gives a
simple asymptotic theory in this particular case.


\begin{lemma} \normalfont
\label{lemma:simpleasym}
Under Assumption~\ref{assumption:truncation} and~$\Delta$ known, for
large~$n$ the estimate~$\hat \theta_n$ is approximately normal
\[
\sqrt{n} (\hat \theta - \theta) \to N(0, \Xi (\theta)^{-1})
\]
where
\[
  \Xi (\theta) = \int_{0}^{\tau} w(s; \theta) \left[ \frac{h^{(1)}(s;
      \theta) \times  h^{(1)} (s;\theta)^{\top}}{h(s; \theta)} \right]
  ds.
\]
and~$\tau$ is the random censoring time of the event process.
\end{lemma}
An estimator for~$\Xi(\theta)$ is
\begin{equation}
\label{eq:fisher}
  \hat \Xi (\theta) = n^{-1} \sum_{i=1}^n \sum_{t \in \bfT_i \cup D_i} 
  w_i(t;\theta) (1-w_i(t;\theta)) \left[ \frac{h_i^{(1)}(t;
      \theta)}{h_i (t; \theta)} \right] \times  \left [
    \frac{h_i^{(1)} (t;\theta)}{h_i(t; \theta)} \right]^\top
\end{equation}
For the log-linear intensity model, the sampling-unbiased estimator
for~$\hat \Xi(\theta)$ is equivalent to the Fisher information for the
previously described logistic regression model.
This implies that subsampling from an inhomogeneous Poisson process,
standard logistic regression software can be used to fit the recurrent
event model by specifying an offset equal to~$\log \pi_i (t)$.
Not only this, Lemma~\ref{prop:optimal} shows
weights~\eqref{eq:waage_weights} are optimal within a particular class
of weighted estimating equations. 

\begin{prop} \normalfont
\label{prop:optimal}
If the event process is an inhomogeneous Poisson point process with
intensity~$h(t; \theta)$ and subsampling occurs via an independent,
inhomogeneous Poisson point process with intensity~$\pi (t)$,
then~$\hat U_n (\theta)$ are optimal estimating functions (i.e., most
efficient) in the class of weighted estimating functions given
by~\eqref{eq:approxscore} replacing~\eqref{eq:waage_weights} by any
weight function~$w_i (t; \theta)$. This class includes the
Horvitz-Thompson estimator under~$w(s; \theta) = 1$.
\end{prop}

\noindent Proposition~\ref{prop:optimal} ensures the only loss of
statistical efficiency is due to subsampling and not using a
suboptimal estimation procedure given subsampling.

\subsection{Computation versus statistical efficiency tradeoff}

Under assumption~\ref{assumption:truncation}, we next consider the
statistical efficiency of our proposed estimator when compared to
complete-data maximum likelihood estimation.
While subsampling introduces additional variation, it may
significantly reduce the overall computational burden. It is this
trade-off that we next make precise.
In particular, we consider the following choice of subsampling
rate,~$\pi(t) = c \times h(t; \theta)$ for $c>0$. That is, the
subsampling rate is proportional to the intensity function with
time-independent constant~$c > 0$.
Under this subsampling rate, the weight
function~\eqref{eq:waage_weights} is equal to $c / (c+1)$.  
Under Lemma~\ref{lemma:simpleasym}, 
\[
\Xi (\theta) = \frac{c}{c+1} \int_0^\tau \frac{ h^{(1)} (t; \theta)
  h^{(1)} (t; \theta)^\top}{h (t; \theta)} dt = \frac{c}{c+1} \Sigma (\theta)
\]
where~$\Sigma(\theta)$ is the Fisher information of the complete-data 
maximum likelihood estimator.
Therefore the relative efficiency is~$c/(1+c)$. For an upper bound~$H
= \max_{t \in (0,\tau)} h(t;\theta)$, if we set~$\pi (t) = c \times
H$, then the relative efficiency can be lower bounded by~$c / (c+1)$.

Recall sensor measurements occur on the order of multiple times per
second.  Suppose the intensity rate is bounded above by $3$ and the
unit time scale was hours. If we can subsample the data at~$30$
times per hour, then we have a lower bound on the efficiency of
$0.909$. For a 4Hz sensor, this reduces the number of samples per hour
from~$4 \times 60 \times 60 = 14,400$ per hour to on average $30$.
While the computational complexity of logistic regression is linear in
the number of samples, we get $480$ times reduction in the data size
at the cost of a $0.909$ statistical efficiency. If we sample~$300$
times per hour, then the efficiency loss is only $0.999$, with a $48$
times reduction in data size. Table~\ref{tab:compvseff} provides
additional examples for a 4Hz sensor.  The data reduction depends on
the sensing rate; however, the lower bound on statistical efficiency
does not.  This is because the subsampling rate only depends on the
upper bound of the intensity function. That is, if the events are rare
then the amount of data we need to subsample is greatly reduced with
no impact to statistical efficiency.

\begin{table}[!th]
\centering
\begin{tabular}{l r r r r r | c}
\multirow{2}{2.5cm}{Subsampling constant ($c$)} 
  & \multicolumn{5}{c}{Upper bound on intensity rate per
    hour} 
  & \multirow{2}{2cm}{Statistical efficiency}\\ \cline{2-6} 
& 0.5 & 1 & 3 & 5 & 10 \\ \hline
1 & 28800.0 & 14400.0 & 4800.0 & 2880.0 & 1440.0 & 0.500 \\
5 & 5760.0 & 2880.0 & 960.0 & 576.0 & 288.0 & 0.833 \\
10 & 2880.0 & 1440.0 & 480.0 & 288.0 & 144.0 & 0.909 \\
100 & 288.0 & 144.0 & 48.0 & 28.8 & 14.4 & 0.990 \\
500 & 57.6 & 28.8 & 9.6 & 5.8 & 2.9 & 0.999 \\ \hline
\end{tabular}
\caption{Data reduction (total number of measurements divided by
  expected number of subsampled measurements) given 4Hz sensor,
  subsampling constant~$c$ and an upper bound on the intensity
  rate per hour~$H$. The subsampling rate is set at~$c \times H$;
  note, lower bound on statistical efficiency only depends on the
  subsampling rate.}   
\label{tab:compvseff}
\end{table}

\section{Theoretical analysis}


\begin{assumption}[Event process assumptions]
\begin{itemize}
\item The subsampling rate~$\pi_i (t) \in [L_\pi, U_\pi]$ 
\item There exists a nonnegative matrix
\item There exists 
\end{itemize}
\end{assumption}

\begin{assumption}[Covariance assumptions]
\begin{itemize}
\item 
\end{itemize}
\end{assumption}

\begin{assumption}[Functional data assumptions]
M1 -- M4 in 
\end{assumption}

\begin{thm}[No reconstruction error]
\label{thm:truncation}
Under regularity conditions A-E in Andersen et al. 1993, 
\[
\frac{n (\hat \theta - \theta) \Xi (\theta) (\hat \theta - \theta) -
  K_n+1}{\sqrt{2 (K_n + 1) }} \to N(0,1).
\]

\end{thm}

\begin{thm}[Reconstruction error OR $\hat \Delta > \Delta$]
\label{thm:biaseddown}



\end{thm}

Theorem~\ref{thm:biaseddown} shows that if we keep~$K_x = K_b = 35$
and therefore have reconstruction error, we can expect a downward bias
in parameter estimates.
Moreover, if~$\Delta < \Delta^\star$ then we can expect a similar bias
in the parameter estimates.  For the former, however, we can simply
let~$K_x = K_b$ grow as a function of data size; for the latter, we
can see if~$\hat \beta(s)$ is statistically significant for~$s$
near~$\Delta$. If so, we can set~$\Delta$ larger to ensure the
condition holds.



\section{Extensions}

In this section, we demonstrate the flexibility of our approach by
exploring extensions in several important directions to ensure these
methods are robust for practical use.
This section will continue to leverage the connection to generalized
functional linear models provided by Lemma~\ref{lemma:logistic}.

\subsection{Missing data}
\label{section:missingdata}

A core concern in mHealth is that sensor data can be missing due to
sensor wearing issue.
To see this, for the suicidal ideation case study, there are XX event
times across the whole dataset.  Of these, XX event times had complete 
data for the prior thirty minutes, XX had missingness less than 10\%
(i.e., sufficient to ), and XX had missingness above 30\%.

The issue here is that~$c_{i,k} (t)$ cannot be estimated if~$X(s,t)$
is not observed for all~$s \in [0,\Delta]$. 
Moreover, we want standard erros to reflect the uncertainty
in these coefficients when missing data is prevalent.
\cite{Goldsmith2015} suggest using best linear unbiased predictors
(BLUP) or posterior modes in the mixed effects model to
estimate~$c_{i,k} (t)$; however, one criticism would be when there is
substantial variability in these estimates.  To deal with this,
\cite{Crainiceanu2010, Goldsmith2010} take a full Bayesian analysis.
\cite{Yao2005} introduced PACE as a frequentist analysis that deals
with this issue.
However,~\cite{Reimherr2018} show that for sparse, irregular
longitudinal, the imputation model should not ignore the outcome~$Y_i
(t)$.

Here we present an extension of~\cite{Reimherr2018} to our setting,
leverage Lemma~\ref{lemma:logistic} and the marginal covariance
estimation procedure to build a multiple imputation approach.
\[
\E [ X_i (s,t) \given Y_i(t) = y, \bfx ] = \mu_y (s,t) + {\bf a}_{i,t}^\top
{\bf B}_i (\bfx_i (t) - \mathbold{\mu}_i )
\]
where~$\mu_y (t) = \E [ X_i (t) \given Y_i (t) = y]$, and
\[
{\bf a}_i^\top = ( \Sigma(s, )
\]
This generalizes the procedure of~\cite{Reimerr2018} to the
longitudinal data setting.
This is possible due to LEmma~\ref{} and our 
Here, we ignore the issue that~$X(t,s)$ is 

The estimate is then given by~$\bar \theta = M^{-1} \sum_{m=1}^M \hat
\theta^{(m)}$; the covariance matrix can be estimated by
\[
M^{-1} \sum_{m=1}^M \hat \Xi^{(m)} (\theta) + \frac{1}{M-1}
\sum_{m=1}^M \hat \theta - \bar \theta) (\hat \theta - \bar \theta)
\]
where~$\hat \Xi^{(m)}$ is the estimator~\eqref{eq:fisher} for
the~$m$th imputated dataset.


\subsection{Multilevel models}

\subsection{Multiple sensors and availability}



\subsection{Re-using estimates under additional sampling}

If~$\pi$ is constant then we can continue to use the same pooled
covariance. 


\section{Simulations}

\subsection{Unbiased estimation}

\subsection{Impact of $\Delta$}

\section{Case Study: }

\bibliographystyle{plainnat}
\bibliography{si-fda-refs}

\appendix

\section{Derivation of the design-unbiased score equations}

\begin{proof}[Proof of Lemma~\ref{lemma:logistic}]
Recall~$\frac{d}{d\theta} \log h_i (t; \theta) = \frac{h^{(1)}_i (t;
  \theta)}{h_i (t; \theta)}$. Therefore, under the log-linear
intensity function, $\frac{d}{d\theta} \log h_i (t; \theta) =
W_{i,t}$. On the other hand,
\[
\frac{d}{d \theta} \log \left( \pi_i (t) + h_i (t;\theta) \right) = 
\frac{\exp \left( W_{i,t}^\prime \theta \right)}{\pi_i (t) + \exp
  \left( W_{i,t}^\prime \theta \right)} W_{i,t}
\]
Therefore,
\begin{align*}
\hat U_n (\theta) 
  &= \sum_{i=1}^n \left \{ \sum_{t \in N_i} W_{i,t} -
    \sum_{t \in N_i \cup D_i} \frac{\exp \left( W_{i,t}^\prime \theta
    \right)}{\pi_i (t) + \exp \left( W_{i,t}^\prime \theta \right)}
    W_{i,t} \right \} \\
  &= \sum_{i=1}^n \left \{ \sum_{t \in N_i} \frac{\pi_i (t)}{\pi_i (t)
    + \exp \left( W_{i,t}^\prime \theta \right)} W_{i,t} -
    \sum_{t \in D_i} \frac{\exp \left( W_{i,t}^\prime \theta
    \right)}{\pi_i (t) + \exp \left( W_{i,t}^\prime \theta \right)}
    W_{i,t} \right \} \\
  &= \sum_{i=1}^n \left \{ \sum_{t \in N_i} w_{i} (t; \theta) W_{i,t} -
    \sum_{t \in D_i} (1 - w_i (t; \theta)) W_{i,t} \right \} \\
  &= - \sum_{i=1}^n \sum_{t \in N_i \cup D_i} \left[ {\bf 1} [t \in
    D_i]  - w_{i} (t; \theta) \right] W_{i,t} \\
  &= \sum_{i=1}^n \sum_{t \in N_i \cup D_i}
    \left[ {\bf 1} [t \in D_i]  - \frac{1}{1 + \exp\left( - (
          \tilde W_{i,t}^\top \theta + \log (\pi_i (t) ) ) \right)}
    \right] \tilde W_{i,t}.
\end{align*}
where~$\tilde W_{i,t} = - W_{i,t}$. This is exactly the score equation
for logistic regression with offset~$\log \pi_i (t)$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:simpleasym}]
Lemma~\ref{lemma:simpleasym} will be proven under regularity
conditions A-E in~\cite[pp. 420--421]{Andersen1993} along with the
following assumptions related to the subsampling rate~$\pi_i (t)$.
For simplicty we assume there exists~$\tau < \infty$ and all censoring
times satisfy~$\tau_i \leq \tau$. Define $R_i (t)$ to be the at risk
indicator for participant~$i$.  That is, 
\[
R_i (t) = \left \{ \begin{array}{c c} 1 & t \in (0,\tau_i) \\ 0
                                        & t > \tau_i\end{array} \right .
\]
We define the joint counting process
\[
M_i (t) = N_i (t) - \int_0^t (h_i (s; \theta) + \pi_i (s)) R_i (s) ds. 
\]
where~$N_i (t)$ is the counting process with jumps at all~$t \in
\bfT_i \cup D_i$.

Asymptotic consistency is guaranteed~\cite[Theorem VI.1.1]{Andersen1993} if
\begin{equation}
\label{eq:andersen1}
n^{-1} \hat U_n (\theta_0 ) \overset{P}{\to} 0,
\end{equation}
and 
\[
n^{-1} \frac{d}{d \theta^{\top}} \hat U_n (\theta) \big |_{\theta =
  \theta_0} = \Xi (\theta),
\]
and
\[
\lim_{n \to \infty} P \left(  and all \theta \in \Theta_0 \right) = 1
\]
To prove~\eqref{eq:andersen1}, we decompose~$\hat U_n (\theta_0)$ into
three terms
\[
n^{-1} \hat U_n (\theta_0) = n^{-1} U_n (\theta_0 )  + n^{-1} \left(
  \tilde U_n (\theta) - U_n (\theta) \right) + n^{-1} \left(\hat U_n
  (\theta) - \tilde U_n (\theta) \right)
\]
where~$\tilde U_n (\theta_0)$ are the logistic score equations with
$\int_0^\infty X_i(t-s) \beta(s) ds$ replaced by the
approximation~$C_{i,t}^\top J_{\phi, \psi} {\bf b}$. 
\cite{Andersen1993}[Theorem VI.1.1] show that~$n^{-1} U_n (\theta_0)
\to 0$. The second term
\begin{align*}
n^{-1} \left( \hat U_n (\theta_0) - U_n (\theta_0) \right) =
  \frac{1}{n} \sum_{i=1}^n \int_0^{\tau} \frac{h^{(1)} (s;
  \theta_0)}{\pi_i (s) + h (s;\theta_0)} d M_i (s). 
\end{align*}
Lenglart's inequality\walt{Need to make precise} implies
the second term converges in probability to zero.  This statement
requires the following two conditions:
\begin{enumerate}
\item The subsampling rate is both lower and upper bounded for all
  ``at risk times''; that is,~$0 < L < \pi_i (t) < U < \infty$ for
  all~$i=1,2,\ldots$ and~$t \in [0,\tau]$. \walt{Could we just bound
    $h_i (t; \theta) + \pi_i (t)$?}
\item There exists a nonnegative definite matrix~$\Xi (\theta)$ such that
\[
n^{-1} \Xi_n (\theta) = n^{-1} \sum_{i=1}^n \int_0^\tau w_i (t;
\theta) (1 - w_i (t; \theta)) \left[ \frac{h_i^{(1)}(t;
      \theta)}{h_i (t; \theta)} \right] \times  \left [
    \frac{h_i^{(1)} (t;\theta)}{h_i(t; \theta)} \right]^\top R_i (t)
  dt \overset{P}{\to} \Xi (\theta).
\] 
\end{enumerate}
The third term satisfies
\begin{align*}
n^{-1} \left \| \hat U_n (\theta_0) - \tilde U_n (\theta_0) \right \|
  &\leq M n^{-1} \sum_{i=1}^n \sum_{t \in N_i \cup D_i}
    \left \| g \left( W_{i,t}^\top \theta + \log (\pi_i (t)
  ) \right) - g \left( \tilde W_{i,t}^\top \theta + \log (\pi_i (t) 
  ) \right) \right \| \\
&= M n^{-1} \sum_{i=1}^n \sum_{t \in N_i \cup D_i}
    \left \| g^\prime \left( W_{i,t}^\top \theta + \log (\pi_i (t) )
  \right) \right\| \times \left \| W_{i,t}^\top \theta_0 - \tilde W_{i,t}^\top
  \tilde \theta_0  \right \| \\
&= \frac{M}{4} \| N_i \cup D_i \| \left ( \sum_{j=p_n+1}^\infty
  \lambda_{j}^2 \sum_{j=q_n+1}^\infty b_j^2 \right)^{1/2}
\end{align*}
where~$M = \sup \| \tilde W_{i,t} \|$,~$g(x) = 1/(1+\exp(-x))$ is the
expit function, and the final line is by Cauchy-Schwarz inequality.


\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:optimal}]
Consider the weighted estimating equations given by~\eqref{eq:approxscore}.
Then
\end{proof}


\end{document}

\item {\bf Step 1}: Estimate~$\hat \mu(s,t)$ via sandwich smoother for
  all~$s \in [0,\Delta]$ and~$t \in \cup_{i=1}^n \left( D_i \cup N_i
  \right)$. Define~$\tilde X (s,t) = X(s,t) - \hat \mu(s,t)$ to be a
  mean-zero process for each~$t$.
\item {\bf Step 2}: Compute the pooled sample covariance
\[
\hat \Xi (s , s^\prime) = 
\left( \sum_{i=1}^n \left | N_i \cup D_i \right |\right)^{-1}
\left( \sum_{i=1}^n \sum_{t \in N_i \cup D_i} \tilde X (s,t) \tilde X
  (s^\prime, t) \right).
\]
Compute at a fine set of grid points. This is an estimate of 
\[
\Xi (s, s^\prime) = \int_{\mathcal{T}} c( (s,T), (s^\prime, T) ) g(T) dT
\]
where~$g(T)$ is a superposition of the event process and the
subsampling process.
\item {\bf Step 3}: Take spectral decomposition~$\{ \hat \phi_k (s),
  \hat \lambda_k \}_{k \geq 1}$. Choose~$K_x \geq K_b$ large enough.
\item {\bf Step 4}: Compute
\[
\hat c_{i,k} (t) = \int_{t-\Delta}^t \tilde X_i (s,t) \hat \phi_k (s) ds
\]
Then
\[
X(s,t) \approx \hat \mu(s,t) + \sum_{k=1}^{K_x} \hat c_{i,k} (t)
\phi_k (s).
\]
\item {\bf Step 5}: Plugging both the spline and KL decomposition we
  have
\begin{align*}
\int_{t-\Delta}^t X(s,t) \beta(s) ds 
  &= \int_{t-\Delta}^t \left[ \hat \mu(s,t) + \sum_{k=1}^{K_x}
    c_{i,t,k} \phi_k (s) \right] \times \left[ \sum_{k^\prime=1}^{K_b}
    \beta_k \psi_k (s) \right] \\
  &= [ M_{i,t}^\prime + C_{i,t}^\prime J_{\phi, \psi} ] \beta
\end{align*}
where~$M_{i,t}$ is a vector with~$i$th entry equal
to~$\int_{t-\Delta}^t \hat \mu(s,t) \psi_k (s) ds$, $C_{i,t} =
(c_{i,t,1}, \ldots, c_{i,t,K_x}) \in \mathbb{R}^{K_x \times 1}$,
$\beta = (\beta_1, \ldots, \beta_{K_b}) \in \mathbb{R}^{K_b \times 1}$
and~$J_{\phi, \psi} \in \mathbb{R}^{K_x \times K_b}$ where the~$(i,j)$
entry is given by
\[
\int_{t-\Delta}^t \phi_{i} (s) \psi_{j} (s) ds.
\]
\item {\bf Step 6}: The approximate score equations
\begin{equation}
\label{eq:weighted_esteq}
\hat U_n (\theta) = \sum_{i=1}^n \left \{ \sum_{t \in N_i} w_{i} (t; \theta) W_{i,t} -
    \sum_{u \in D_i} (1 - w_i (u; \theta)) W_{i,u} \right \} 
\end{equation}
where
\begin{equation}
\label{eq:waage_weight}
w_i (t; \theta) = \frac{h_i(t;\theta)}{\pi_i (t) + h_i (t;\theta)}.
\end{equation}
Under weights~\eqref{eq:waage_weight} and log-linear intensity
function,~\eqref{eq:weighted_esteq} is the 
\end{itemize}

