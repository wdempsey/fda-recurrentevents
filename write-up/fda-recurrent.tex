\documentclass[11pt]{amsart}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}
\usepackage{amsfonts,amssymb,amsthm, txfonts, pxfonts,amscd}
\usepackage[stretch=10]{microtype}
%\usepackage{hyperref}
\def\struckint{\mathop{%
\def\mathpalette##1##2{\mathchoice{##1\displaystyle##2}%
 {##1\textstyle##2}{##1\scriptstyle##2}{##1\scriptscriptstyle##2}}%
\mathpalette
{\vbox\bgroup\baselineskip0pt\lineskiplimit-1000pt\lineskip-1000pt
\halign\bgroup\hfill$}
{##$\hfill\cr{\intop}\cr\diagup\cr\egroup\egroup}%
}\limits}
\usepackage{natbib}
\usepackage{color}
\usepackage{booktabs,caption,fixltx2e}
\usepackage[flushleft]{threeparttable}

\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}
\usepackage{amsfonts,amssymb,amsthm, txfonts, pxfonts,amscd}
%\usepackage[stretch=10]{microtype}
%\usepackage{hyperref}
%\def\struckint{\mathop{%
%\def\mathpalette##1##2{\mathchoice{##1\displaystyle##2}%
% {##1\textstyle##2}{##1\scriptstyle##2}{##1\scriptscriptstyle##2}}%
%\mathpalette
%{\vbox\bgroup\baselineskip0pt\lineskiplimit-1000pt\lineskip-1000pt
%\halign\bgroup\hfill$}
%{##$\hfill\cr{\intop}\cr\diagup\cr\egroup\egroup}%
%}\limits}

\newcommand{\sam}[1]{{\color{blue}{#1}}}
\newcommand{\walt}[1]{\textcolor{red}{[WD:\ #1]}}

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}
\setlength{\topmargin}{-0.25in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
%\setlength{\evensidemargin}{.25in}
%\setlength{\oddsidemargin}{.25in}


\usepackage{setspace}
\doublespacing

\usepackage{soul,color, mathtools}

\def\pr{\mathop{\text{pr}}\nolimits}

\def\Bka{{\it Biometrika}}
\def\AIC{\textsc{aic}}
\def\T{{ \mathrm{\scriptscriptstyle T} }}
\def\v{{\varepsilon}}
\def\partitionsn{\mathop{\mathcal{P}_{[n]}}\nolimits}
\def\partitionsN{\mathop{\mathcal{P}_{\infty}}\nolimits}
\def\Dmn{\mathop{{D}_{m,n}}\nolimits}
\def\symmetricn{\mathop{\mathcal{S}_{n}}\nolimits}
\def\PE{\mathop{\rm Pitman\mbox{-}Ewens}\nolimits}
\def\per{\mathop{\rm per}\nolimits}
\def\U{\mathop{\mathcal{U}_{}}\nolimits}
\def\Nb{\mathop{\mathbb{N}_{}}\nolimits}
\def\Nbb{\mathop{\mathbf{N}_{}}\nolimits}
\def\nbb{\mathop{\mathbf{n}_{}}\nolimits}
\def\Xbb{\mathop{\mathbf{X}_{}}\nolimits}
\def\fin{\mathop{\text{fin}}\nolimits}
\def\pr{\mathop{\text{pr}}\nolimits}
\def\E{\mathcal{E}}
\def\G{\mathcal{G}}
\def\deg{\text{deg}_{}}
\def\equalinlaw{=_{\mathcal{D}}}
\def\Fk{\mathop{\mathcal{F}_k^{\downarrow}}\nolimits}
\def\F{\mathop{\mathcal{F}^{\downarrow}}\nolimits}
\def\size{\mathop{\text{size}}\nolimits}
\def\Ycong{\mathop{\EY}\nolimits}
\def\EIsig{\mathop{\mathcal{E}_{\mathcal{I}}^{\sigma}}\nolimits}
\def\EY{\mathop{\mathcal{E}_{Y}}\nolimits}
\def\finp{\mathop{\fin(\mathcal{P})}\nolimits}
\def\En{\mathop{\mathfrak{E}_{[n]}}\nolimits}
\def\EN{\mathop{\mathfrak{E}_{\Nb}}\nolimits}
\def\logit{\mathop{\text{logit}_{}}\nolimits}
\def\EI{\mathop{\mathcal{E}_{\mathcal{I}}}\nolimits}
\def\ES{\mathop{\mathfrak{E}_S}\nolimits}
\def\Ifk{\mathop{{I}_{}}\nolimits}
%\def\mathcal{I}{\mathop{\mathcal{I}_{}}\nolimits}
\def\Ycong{\mathop{\EY}\nolimits}
\def\EIsig{\mathop{\mathcal{E}_{\mathcal{I}}^{\sigma}}\nolimits}
\def\EY{\mathop{\mathcal{E}_{Y}}\nolimits}
\def\finp{\mathop{\fin(\mathcal{P})}\nolimits}
\def\En{\mathop{\mathfrak{E}_{[n]}}\nolimits}
\def\EN{\mathop{\mathfrak{E}_{\Nb}}\nolimits}
\def\logit{\mathop{\text{logit}_{}}\nolimits}
\def\EI{\mathop{\mathcal{E}_{\mathcal{I}}}\nolimits}
\def\ES{\mathop{\mathfrak{E}_S}\nolimits}



\DeclareRobustCommand{\citeext}[1]{\citeauthor{#1} (\citeyear{#1})}
\DeclareRobustCommand{\citeint}[1]{(\citeauthor{#1}, \citeyear{#1})}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{meas}[thm]{Measurement}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{assumption}[thm]{Assumption}
\newtheorem{rmk}[thm]{Remark}%\endlocaldefs

% change default numbering for enumerate environment to be in parentheses

\makeatletter

\def\S{\mathcal{S}}
\def\indep{\mathrel{\rlap{$\perp$}\kern1.6pt\mathord{\perp}}}
\def\R{\mathcal{R}}
\def\H{\mathcal{H}}
\def\E{\mathbb{E}}
\def\Y{{\bf Y}}
\def\Cov{\text{Cov}}
\def\one{{\bf 1}}
\def\diag{\text{diag}}
\def\given{\, | \,}
\def\Given{\, \big | \,}
\def\Nat{\mathbb{N}}
\def\Real{\mathbb{R}}
\def\bft{{\bf t}}
\def\bfx{{\bf x}}
\def\bfp{{\bf p}}
\def\bfT{{\bf T}}
\def\dotminussym#1#2{%
  \setbox0=\hbox{$\m@th#1-$}%
  \kern.5\wd0%
  \hbox to 0pt{\hss\hbox{$\m@th#1-$}\hss}%
  \raise.6\ht0\hbox to 0pt{\hss$\m@th#1.$\hss}%
  \kern.5\wd0}
\newcommand{\dotminus}{\mathbin{\mathpalette\dotminussym{}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\mathchardef\mhyphen="2D

% display breaks


\begin{document}


\title{Recurrent event analysis in the presence of real-time high
  frequency data via random subsampling}
\author{Walter Dempsey}
\address {Department of Statistics, 
  Harvard University, One Oxford Street
   Cambridge, MA  02138, USA}
 \email{wdempsey@fas.harvard.edu}

\date{\today}

\begin{abstract}
Digital monitoring studies collect real-time high frequency data via
mobile sensors in the subjects' natural environment.  
This data can be used to model the impact of changes in physiology on
recurrent event outcomes such as smoking, drug use, alcohol use, or
self-identified moments of suicide ideation. 
Likelihood calculations for the recurrent event analysis, however,
become computationally prohibitive in this setting.  
% Unlike classic joint models, event risk is a function of
% recent sensor data, not simply current sensor values.  
Motivated by this, a random subsampling framework is proposed for
computationally efficient, approximate likelihood-based estimation.
A subsampling-unbiased estimator for the derivative of the cumulative
hazard enters into an approximation of log-likelihood. The estimator
has two sources of variation: the first due to the recurrent event
model and the second due to subsampling. The latter can be reduced by
increasing the sampling rate; however, this leads to increased
computational costs.  The approximate score equations are equivalent
to logistic regression score equations, allowing for standard,
``off-the-shelf'' software to be used in fitting these models.
Simulations demonstrate the method and efficiency-computation
tradeoff. We then illustrate the approach using data from a digital
monitoring study of suicide ideation.
\end{abstract}

\keywords{recurrent events; probabilistic subsampling; estimating equations;
high frequency time series; logistic regression}


\maketitle

\section{Introduction}

Advancement in mobile technology has led to the rapid integration of
mobile and wearable sensors into behavioral health~\citep{Freeetal2013}.
Take HeartSteps, for example, a mobile health (mHealth) study designed
to increase physical activity in sedentary adults
\citep{KlasnjaHS2019}. Here, a Jawbone sensor is used to monitor step
count every minute of the participant's study day.
Of interest in many mHealth studies is the relation of such
real-time high frequency sensor data to an adverse, recurrent event
process. In a smoking cessation mHealth study~\citep{Sense2Stop}, for
example, the relation between a time-varying sensor-based measure of
physiological stress and smoking lapse is of scientific interest.
In a suicidal ideation mHealth study~\citep{Kleiman2018}, the relation of
electrodermal activity (EDA) and accelerometer with self-identified
moments of suicidal ideation is of scientific interest.


The goal of this paper is two-fold: (1) to discuss the appropriate
choice of statistical model for joint high-frequency sensor data and
the recurrent event data, and (2) to construct a robust,
easy-to-implement method for parameter estimation and inference. 
For (1), we discuss an important issue regarding measurement-error
models when paired with recurrent event outcomes.
For (2), we introduce a random subsampling procedure that has
several benefits.  First, the resulting inference is unbiased;
however, there is a computation-efficiency trade-off. In particular, a
higher sampling rate can decrease variance at the cost of increased
computation.  We show via simulations that the benefits of incredibly
high sampling rates is often negligible, as the contribution to the
variation is small in relation to to variation in the underlying
stochastic processes. Second, optimal estimating equations are derived
that depend on the instantaneous rate of the recurrent event process,
which increases efficiency of the subsampling procedure. 
Finally, implementation can leverage existing, standard software for
functional data analysis and logistic regression, leading to fast
adoption by domain scientists. 

\section{Recurrent event process and associated high frequency data}

Suppose $n$ subjects are independently sampled with observed event
times~$\bfT_{i} = \{ T_{i,1}, \ldots, T_{i,k_i}\}$ over some observation
window $[0, \tau_i]$ for each subject $i = 1,\ldots, n$.  Assume the
event times are ordered, i.e., $T_{i,j} < T_{i,j^\prime}$ for $j <
j^\prime$. The window length, $\tau_i$, is the censoring time and is
assumed independent of the event process. Let~$N_{i} (t)$ denote the
associated counting process of $\bfT_{i}$; that is, $N_i (t) =
\sum_{j=1}^{k_i} 1 [ T_{i,j} < t ]$.  
In this section, we assume a single-dimensional health process~$\bfx_i
= \{ x_i (s) \}_{0 < s < \tau_i}$ for each participant is measured at
a dense grid of time points.  Accelerometer, for example, is measured
at a rate of 32Hz (i.e., $32$ times per second). Electrodermal
activity (EDA), on the other hand, is measured at a rate of 4Hz (i.e.,
4 times per second).  Given the high frequency nature of the sensor 
measurements, this paper assumes the process is measured continuously.   

Let~$H_{i,t}^{NX} = H_{i,t}^{N} \otimes H_{i,t}^{X}$ be the $\sigma$-field
generated by all past values~$(N_i (s), x_i (s))_{0 \leq s \leq t}$.
The instantaneous risk of an event at time~$t$ is assumed to depend on
the health process, time-in-study, and the event history through a
fully parametric conditional hazard function:
\begin{equation}
\label{eq:hazard}
h_i \left( t \Given H_{i,t}^{NX} ; \theta \right) =
\lim_{\delta \to 0} \delta^{-1} \pr \left( N_i(t+\delta) - N_i(t) > 0
  \given H_{i,t}^{NX}; \theta \right), 
\end{equation}
where~$\theta$ is the parameter vector. Equation~\eqref{eq:hazard}
states the hazard may depend on past values of the event history and
covariates.  For high frequency physiological data, we assume that 
current risk is log-additive and depends on a linear functional of the
health process over some recent window of time and some pre-specified
features of the counting process; that is,  
\begin{equation}
\label{eq:hazardlinear}
h_i \left( t \given  H_{i,t}^{NX} ; \theta \right) = 
h_0 (t) \exp \left( g_t \left( H_{i,t}^{N} \right)^{\prime} \alpha
  + \int_{t-\Delta}^t x_i (s) \beta(s) ds  \right)
\end{equation}
where~$\Delta$ is an unknown window-length, and $g_t( H_{i,t}^N ) \in
\mathbb{R}^p$ is a $p$-length feature vector summarizing the
event-history and time-in-study information. The final
term~$\int_{t-\Delta}^t x_i(s) \beta(s) ds$ reflects the unknown
linear functional form of the impact of the time-varying covariate on
current risk.   

An alternative to~\eqref{eq:hazardlinear} would be to construct
features from the sensor data history~$f_t ( H_{i,t}^{X}) \in
\mathbb{R}^q$ and incorporated these features in the place of the
final term. Our current approach builds linear features of~$H_{i,t}^X$
directly from the integrated history, avoiding the feature
construction problem -- a highly nontrivial issue for high frequency
time-series data.  The main caveat is the additional
parameter~$\Delta$; however, we show that so long as the
estimated~$\hat \Delta$ exceeds~$\Delta$, then resulting estimation is
unbiased albeit at a loss of efficiency.  Moreover, sensitivity
analysis can be peformed to determine how choice of $\hat \Delta$
affects inference.  One limitation of the approach presented here is
that only fully parametric hazard models may be fit to the data.
However, a spline model for the log baseline hazard affords sufficient
model flexibility.

\subsection{Measurement-error models with event processes}

One potential criticism for~\eqref{eq:hazard} is that the health
process may be measured with error.  
A common mathematical strategy for joint models is to consider an
unobservable, latent process~$\eta_i$ such that~$\bfT_i \indep x_i
\given \eta_i$. Take~$\eta_i$ to be a zero-mean Gaussian process with 
\begin{equation}\label{eq:jm}
x_i(t) = \eta_i (t) + \epsilon_i (t),\quad \text{ and } \log h_i (t
\given \eta ) = \log h_0 (t) + g_t \left( H_{i,t}^N \right)^{\prime}
\alpha + \int_{t-\Delta}^t \eta_i (s) \beta (s) ds
\end{equation}
where~$\epsilon (t)$ is a white-noise measurement error. Thus,
$\eta_i (t)$ is the ``true and unobserved value of the longitudinal
outcome'' \citep[Sec. 2.1, pp.3]{Rizopoulos2010}. The conditional intensity
function given~$\bfx_i$ is given by
\[
\pr \left ( T_{i,j} = t \Given \H_{i,\tau_i}^X, \{ T_{i,j^\prime} < t \}_{j^\prime <
  j}, T_{i,j} \geq t \right) = E \left( h_i (t \given \eta) \exp
\left( \int_{T_{i,j-1}}^t h_i (\eta (s)) ds \right) \given \bfx_i
\right).
\]
Therefore, it appears that the conditional hazard function depends 
not only on past $x$-values, but also on future $x$-values.
Therefore, the processes~$\bfx$ and~$\bfT$ do not satisfy
\emph{independent evolution}~\citep{DempseyPMCC2}.
This is quite unnatural, as~\eqref{eq:jm} suggests current risk
depends on future values of the sensor process.
To alleviate this issue, we employ the strategy of taking~$\epsilon
(t) \equiv 0$ and treating~$\bfx_i$ as a mean-zero Gaussian process.
The proposed inferential procedure only relies on the Gaussian
assumption when~$\bfx$ is observed with missingness (see
Section~\ref{section:missingdata}).

\subsection{Likelihood calculation}  

For the sake of notational simplicity, we leave the dependency of the
conditional hazard function on~$H_{i,t}^{NX}$ implicit, and write~$h_i
(t; \theta)$. The component of the log-likelihood related to the event
process is then given by
\[
  L_n (\theta) = \sum_{i=1}^{n} \left ( \sum_{j=1}^{k_i} 
    \log \left( h_i \left( T_{i,j}; \theta
      \right) \right) - H_{i} \left( \tau_i; \theta \right) \right) 
\]
where~$H_{i} (\tau_i ; \theta) = \int_{0}^{\tau_i} h_{i} (t; \theta)
dt$. Solving the associated score equations~$U_n (\theta) = {\bf 0}$
yields the maximum likelihood estimator~$\hat \theta$, where
\[
U_n (\theta) = \sum_{i=1}^{n} \left ( \sum_{j=1}^{k_i} \frac{h^{(1)}_i
    (T_{i,j}; \theta)}{h_i (T_{i,j}; \theta)} - H^{(1)}_{i} (\tau_i;
  \theta) \right)  
\]
where~$h_i^{(1)} (T_{i,j}; \theta)$ and $H_i^{(1)} (\tau_{i}; \theta)$
are derivatives with respect to $\theta$.

In classical joint models~\citep{Henderson2000, Tsiatis2004},
time-varying covariates~$x_i (t)$ are observed only intermittently at
appointment times. In our current setting, maximizing the likelihood
is computationally prohibitive since for any~$\theta$ we must compute
the cumulative hazard functions~$H_{i} (\tau_i; \theta)$ which require
integration of~$h_i(t;\theta)$ given by~\eqref{eq:hazardlinear} which
itself depends on the integral~$\int_{t-\Delta}^t x_i (s) \beta(s)
ds$.  That is, the risk model now depends on an integrated past
history of the time-varying covariate which leads to severe increase
in computational complexity. 

\subsection{Probabilistic subsampling framework}

To solve the computational challenge we employ a point-process
subsampling design to obtain unbiased estimates of the derivative of
the cumulative hazards for each subject. The subsampling procedure
treats the collected sensor data as a set of \emph{potential
  observations}. Suppose covariate information is sampled at times
drawn from an independent inhomogeneous Poisson point process with
known intensity~$\pi_i (t)$. At a subsampled time~$t$, the
\emph{windowed covariate history} $\{ x_i (t-s)\}_{0 \leq s \leq
  \Delta}$ and counting process features~$g_t (H_{i,t}^N)$ are
observed. Optimal choice of~$\pi_i (t)$ is beyond the scope of this
paper; however, the literature suggests setting proportional to the
hazard function~$h_i (t; \theta)$. 

An estimator is design-unbiased if its expectation is equal to that
parameter under the probability distribution induced by the sampling
design~\citep{Cassel1977}. Let~$D_i \subset [0,t_i]$ denote the set of
subsampled points.  Note, by design, this set is distinct from the set
of event times, i.e.,~$\bfT_i \cap D_i = \emptyset$.  Under
subsampling via $\pi_i (t)$, one may compute a Horvitz-Thompson
estimator of the derivative of the cumulative hazard:
\[
\hat H_{i}^{(1)} (\tau_i; \theta) = \sum_{u \in D_i} \frac{
  h^{(1)}_i (u; \theta) }{ \pi_i (s) }
\]
An alternative design-unbiased estimator of the derivative of the
cumulative hazards is given by
\begin{equation}
\label{eq:WPest}
\hat H_{i}^{(1)} (\tau_i; \theta) = \sum_{u \in (\bfT_i \cup D_i)}
\frac{ h^{(1)}_i ( t; \theta ) }{ \pi_i (s) + h_i ( t ; \theta) }
\end{equation}
Equation~\eqref{eq:WPest} is the estimator suggested
by~\cite{Waagepetersen2008}.  This estimator depends on the
superposition of the event and subsampling processes.
Proposition~\ref{prop:optimal} shows the estimator for~$\theta$
associated with using~\eqref{eq:WPest} is the most efficient within a
suitable class of estimators for the derivative of the cumulative
hazard function (including the Horvitz-Thompson estimator).
Therefore, we restrict our attention to~\eqref{eq:WPest} for the
remainder of this paper. Letting
\[
w_i (t; \theta) = \frac{\pi_i (t)}{\pi_i (t) + h_i (t ; \theta)},
\]
the resulting estimating equations can be written as
\begin{equation}
\label{eq:approxscore}
\hat{U}_n (\theta) = \sum_{i=1}^n \left[ \sum_{u \in \bfT_i} w_i(u; \theta)
  \frac{h^{(1)} (u; \theta)}{ h ( t; \theta)}  - \sum_{u \in D_i} w_i(u;
  \theta) \frac{h_i^{(1)} (t; \theta)}{ \pi_i (t) } \right].
\end{equation}
Equation~\eqref{eq:approxscore} are approximate score functions built
via plug-in of the design-unbiased estimator of the derivative of the
cumulative hazard given in~\eqref{eq:WPest}.

\section{Longitudinal functional principal components within
  event-history analysis}

Probabilistic subsampling converts the single sensor stream~$\bfx_i$
into a sequence of functions observed repeatedly at sampled
times~$D_i$ and event times~$\bfT_i$. Such a data structure is
commonly referred to as \emph{longitudinal functional
  data}~\citep{Xiao2013, GoldSmith2015}.
Given the large increase in longitudinal functional data in recent
years, corresponding analysis has received much recent
attention~\citep{Morris2003, MorrisCarroll2006,
  Baladandayuthapani2008, Di2009, Greven2010, Staicu2010,
  ChenMuller2012, LiGuan2014}.
Here, we combine work by~\cite{Park2018} and~\cite{Goldsmith2011} to
construct a computationally efficient penalized functional method for
solving the estimation equations~$\hat U_n (\theta)$.

We start by defining~$X(t,s) = x(t-s)$ to be the sensor measurement~$0
\leq s \leq \Delta$ time units prior to time~$t \in \bfT_i \cup D_i$.
We use the sandiwch smoother~\citep{Xiao2013} to estimate the
mean~$\mu(s,t) = \E [ X(t,s)]$.
Alternative bivariate smoothers exist, such as the kernel-based local
linear smoother~\citep{Hastie2009} bivariate tensor product
splines~\cite{Wood2006} and the bivar
The sandwich smoother was chosen for its computational efficiency and
estimation accuracy.
We then define~$\tilde X(s,t) = X(s,t) - \hat \mu(s,t)$ to be the
mean-zero process at each time~$t \in $.


Next, 



Use of the marginal covariance~\cite{Park2018} reduces allows us to avoid
computationally prohibitive computations.




\begin{itemize}
\item {\bf Step 1}: Estimate~$\hat \mu(s,t)$ via sandwich smoother for
  all~$s \in [0,\Delta]$ and~$t \in \cup_{i=1}^n \left( D_i \cup N_i
  \right)$. Define~$\tilde X (s,t) = X(s,t) - \hat \mu(s,t)$ to be a
  mean-zero process for each~$t$.
\item {\bf Step 2}: Compute the pooled sample covariance
\[
\hat \Xi (s , s^\prime) = 
\left( \sum_{i=1}^n \left | N_i \cup D_i \right |\right)^{-1}
\left( \sum_{i=1}^n \sum_{t \in N_i \cup D_i} \tilde X (s,t) \tilde X
  (s^\prime, t) \right).
\]
Compute at a fine set of grid points. This is an estimate of 
\[
\Xi (s, s^\prime) = \int_{\mathcal{T}} c( (s,T), (s^\prime, T) ) g(T) dT
\]
where~$g(T)$ is a superposition of the event process and the
subsampling process.
\item {\bf Step 3}: Take spectral decomposition~$\{ \hat \phi_k (s),
  \hat \lambda_k \}_{k \geq 1}$. Choose~$K_x \geq K_b$ large enough.
\item {\bf Step 4}: Compute
\[
\hat c_{i,k} (t) = \int_{t-\Delta}^t \tilde X_i (s,t) \hat \phi_k (s) ds
\]
Then
\[
X(s,t) \approx \hat \mu(s,t) + \sum_{k=1}^{K_x} \hat c_{i,k} (t)
\phi_k (s).
\]
\item {\bf Step 5}: Plugging both the spline and KL decomposition we
  have
\begin{align*}
\int_{t-\Delta}^t X(s,t) \beta(s) ds 
  &= \int_{t-\Delta}^t \left[ \hat \mu(s,t) + \sum_{k=1}^{K_x}
    c_{i,t,k} \phi_k (s) \right] \times \left[ \sum_{k^\prime=1}^{K_b}
    \beta_k \psi_k (s) \right] \\
  &= [ M_{i,t}^\prime + C_{i,t}^\prime J_{\phi, \psi} ] \beta
\end{align*}
where~$M_{i,t}$ is a vector with~$i$th entry equal
to~$\int_{t-\Delta}^t \hat \mu(s,t) \psi_k (s) ds$, $C_{i,t} =
(c_{i,t,1}, \ldots, c_{i,t,K_x}) \in \mathbb{R}^{K_x \times 1}$,
$\beta = (\beta_1, \ldots, \beta_{K_b}) \in \mathbb{R}^{K_b \times 1}$
and~$J_{\phi, \psi} \in \mathbb{R}^{K_x \times K_b}$ where the~$(i,j)$
entry is given by
\[
\int_{t-\Delta}^t \phi_{i} (s) \psi_{j} (s) ds.
\]
\item {\bf Step 6}: The approximate score equations
\begin{equation}
\label{eq:weighted_esteq}
\hat U_n (\theta) = \sum_{i=1}^n \left \{ \sum_{t \in N_i} w_{i} (t; \theta) W_{i,t} -
    \sum_{u \in D_i} (1 - w_i (u; \theta)) W_{i,u} \right \} 
\end{equation}
where
\begin{equation}
\label{eq:waage_weight}
w_i (t; \theta) = \frac{h_i(t;\theta)}{\pi_i (t) + h_i (t;\theta)}.
\end{equation}
Under weights~\eqref{eq:waage_weight} and log-linear intensity
function,~\eqref{eq:weighted_esteq} is the logistic score function for
the binary variable~$Y_i(u)$ for $u \in N_i \cup D_i$ and~$i \in [n]$
where~$Y_i(u) = 1[u \in N_i]$. To see this, we rewrite $\hat U_n
(\theta)$ as 
\[
  - \sum_{i=1}^n \sum_{t \in N_i \cup D_i}
    \left[ {\bf 1} [t \in D_i]  - w_{i} (t; \theta) \right] W_{i,t} 
= \sum_{i=1}^n \sum_{t \in N_i \cup D_i}
    \left[ {\bf 1} [t \in D_i]  - \frac{1}{1 + \exp\left( - (
          \tilde W_{i,t}^\top \theta + \log (\pi_i (t) ) ) \right)}
    \right] \tilde W_{i,t}.
\]
where~$\tilde W_{i,t} = - W_{i,t}$.
This is equivalent to
\[
\sum_{i=1}^n y_i \log p (x_i)
\]
\end{itemize}

\begin{thm}[Asymptotic theory]
For large~$n$, the estimate~$\hat \theta_n$ is approximately normal
\[
\sqrt(n) (\hat \theta_n - \theta) \to N(0, \Xi (\theta)^{-1})
\]
where
\[
  \Xi (\theta) = \int_{0}^{\tau} w(s; \theta) \left[ \frac{h^{(1)}(s;
      \theta) \times  h^{(1)} (s;\theta)^{\top}}{h(s; \theta)} \right]
  ds.
\]
and~$\tau$ is a random 
\end{thm}
An estimator for~$\Xi(\theta)$ is
\[
  \hat \Xi (\theta) = n^{-1} \sum_{i=1}^n \sum_{u \in N_i \cup D_i} 
  w_i(u;\theta) (1-w_i(u;\theta)) \left[ \frac{h_i^{(1)}(u;
      \theta)}{h_i (u; \theta)} \right] \times  \left [ \frac{h_i^{(1)} (u;\theta)}{h_i(u; \theta)} \right]^\top
\]
For the Cox model, the sampling-unbiased estimator for~$\hat
\Xi(\theta)$ is equivalent to the Fisher information for the
previously described logistic regression model.
This implies that subsampling from an inhomogeneous Poisson process,
standard logistic regression software can be used to fit the recurrent
event model by specifying an offset equal to~$\log (\pi_i (s))$.

\begin{prop}[Optimality of proposed approach]
\label{prop:optimal}
If the event process is an inhomogeneous Poisson point process with
intensity~$h(t; \theta)$ and subsampling occurs via an independent,
inhomogeneous Poisson point process with intensity~$\pi (t)$,
then~$\hat U_n (\theta)$ are optimal estimating functions in a class
of weighted estimating functions given by~\eqref{eq:weighted_esteq}
with replacing~\eqref{} by any weight function~$w_i (t; \theta)$. This
class includes the Horvitz-Thompson estimator under~$w(s; \theta)
= 1$.
\end{prop}

Proposition~\ref{} follows immediately from~\cite{Rathbun2013}.
This ensures the only loss of statistical efficiency is related to the 
subsampling procedure and not the associated methodology.

\subsection{Computation versus statistical efficiency tradeoff}

Under~$\pi(t) = c \times h(t; \theta)$ for $c>0$,
$w(t; \theta) = c/(c+1)$.  Then
\[
\Xi (\theta) = \frac{c}{c+1} \int_0^\tau \frac{ h^{(1)} (t; \theta)
  h^{(1)} (t; \theta)^\top}{h (t; \theta)} dt = \frac{c}{c+1} \Sigma (\theta)
\]
Therefore the relative efficiency compared to the asymptotic variance
of the maximum likelihood estimator is~$1 + 1/c$.
Suppose we have~$\pi (t) = c \times \max_{t \in (0,\infty)} h(t;
\theta)$, then~$w(t;\theta) \leq c/(c+1)$.  this implies
\[
\Xi (\theta) \leq \frac{c}{c+1} \Sigma(\theta)
\]
which again leads to an upper bound on efficiency of~$c/(1+c)$.
Recall that the sampling rate is on the order of multiple measurements
per second.  Suppose the risk is bounded above by $3$ (i.e., $3$ 
events per hour). Then, we can sample~$30$ times per hour and have
efficiency of $0.91$; for a 4Hz sensor, this reduces the number of
samples from~$4 \times 60 \times 60 = 14,400$ per hour to on average
$30$.  Even if the computational complexity is linear in the number of
samples, we get $480$ times reduction in the data size at the cost of
a $0.91$ statistical efficiency. 
If we sample~$300$ times per hour, then the efficiency loss is only
$0.99$, with a $48$ times reduction in data size.

\subsection{Time-varying coefficient and prediction}

Replace~$\beta(s)$ by~$\beta_t (s)$ and assume it's smooth in~$t$.

THIS IS WHERE PREDICTION IS IMPORTANT!!

We wish to estimate the risk at non-sampled points.

For prediction, we use sparse FPCA (Yao 2005) 
\begin{itemize}
\item Take $\hat c_{k} (s,t)$ for~$t \in N \cup D$ and~$s \in
  [0,\Delta]$
\end{itemize}


Take $\{ X (s) \}_{0 < s < \Delta}$. Let~$\psi_{j} (s)$ is an
orthonormal basis for $L^2 [ 0,1 ]$.  Then

\[
X(s) - \mu(s) 
\]

\[
- b^\prime D b
\]

\section{Theoretical analysis}

\begin{assumption}[Event process assumptions]

\end{assumption}

\begin{assumption}[Covariance assumptions]

\end{assumption}

\begin{assumption}[Functional data assumptions]

\end{assumption}

\begin{thm}[No reconstruction error]

\end{thm}

\begin{thm}[Reconstruction error but~$\hat \Delta > \Delta$]
\label{eq:biaseddown}
\end{thm}

Theorem~\ref{eq:}



\begin{thm}[When~$\hat \Delta < \Delta$] 

\end{thm}

\section{Extensions}

\subsection{Re-using estimates under additional sampling}
Inverse weights can be used to build new pooled covariance.
Weights are set to
\[
\frac{\pi_{\text{new}} (t) + \hat \lambda (t; \theta)}{\pi_{\text{old}} (t) + \hat \lambda (t; \theta)}
\]

\subsection{Missing data}
\label{section:missingdata}

\subsection{Multilevel models}

\subsection{Multiple sensors and availability}

\section{Simulations}

\subsection{Unbiased estimation}

\subsection{Impact of $\Delta$}

\section{Case Study: }

\bibliographystyle{plainnat}
\bibliography{si-fda-refs}

\appendix

\section{Derivation of the design-unbiased score equations}

We re-write the hazard function as:
\[
h_i (t; \theta) = \exp \left( Z_{i,t}^\prime, \alpha + [M_{i,t} + C_{i,t}^\prime
  J_{\phi, \psi} ] \beta \right) = \exp \left( W_{i,t}^\prime \theta \right),
\]
where~$W_{i,t}^\prime = [ Z_{i,t}^\prime (M_{i,t}^\prime +
C_{i,t}^\prime J_{\phi, \psi}) ]$ and~$\theta = (\alpha,
\beta)$. Then, the score function is:
\[
\hat U_n (\theta) = \sum_{i=1}^n \left \{  \sum_{t \in N_i}
  \frac{h_i^{(1)} (t; \theta)}{h_i (t; \theta)} - \hat H_i^{(1)}
  (\tau_i, 0; \theta) 
\right \}
\]
where
\[
\hat H_i^{(1)} (\tau_i; \theta) = \sum_{u \in N_i \cup D_i} \frac{
  h^{(1)}_i (u; \theta)}{\pi_i (u) + h_i (u; \theta)}.
\]
Note that
\[
\frac{d}{d\theta} \log h_i (t; \theta) = \frac{h^{(1)}_i (t;
  \theta)}{h_i (t; \theta)}.
\]
Therefore, these terms have very simple forms!!! 
\[
\frac{d}{d\theta} \log h_i (t; \theta) = W_{i,t}
\]
On the other hand,
\[
\frac{d}{d \theta} \log \left( \pi_i (t) + h_i (t;\theta) \right) = 
\frac{\exp \left( W_{i,t}^\prime \theta \right)}{\pi_i (t) + \exp
  \left( W_{i,t}^\prime \theta \right)} W_{i,t}
\]
Therefore,
\begin{align*}
\hat U_n (\theta) 
  &= \sum_{i=1}^n \left \{ \sum_{t \in N_i} W_{i,t} -
    \sum_{u \in N_i \cup D_i} \frac{\exp \left( W_{i,u}^\prime \theta
    \right)}{\pi_i (t) + \exp \left( W_{i,u}^\prime \theta \right)}
    W_{i,u} \right \} \\
  &= \sum_{i=1}^n \left \{ \sum_{t \in N_i} \frac{\pi_i (t)}{\pi_i (t)
    + \exp \left( W_{i,u}^\prime \theta \right)} W_{i,t} -
    \sum_{u \in D_i} \frac{\exp \left( W_{i,u}^\prime \theta
    \right)}{\pi_i (t) + \exp \left( W_{i,u}^\prime \theta \right)}
    W_{i,u} \right \} \\
  &= \sum_{i=1}^n \left \{ \sum_{t \in N_i} w_{i} (t; \theta) W_{i,t} -
    \sum_{u \in D_i} (1 - w_i (u; \theta)) W_{i,u} \right \} \\
\end{align*}


\end{document}